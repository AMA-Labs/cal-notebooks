{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Animated Landsat time series</u> <img align=\"right\" src=\"../resources/csiro_easi_logo.png\">\n",
    "\n",
    "**Contents**\n",
    "  - [Overview](#Overview)\n",
    "  - [Notebook setup](#Notebook-setup)\n",
    "    - [Importing modules and functions](#Importing-modules-and-functions)\n",
    "    - [Setting up the analysis variables](#Setting-up-the-analysis-variables)\n",
    "  - [Loading up the Landsat-8 time series](#Loading-up-the-Landsat-8-time-series)\n",
    "    - [DataCube query](#DataCube-query)\n",
    "      - [Satellite product](#Satellite-product)\n",
    "      - [Spatial extents](#Spatial-extents)\n",
    "      - [Data query and load](#Data-query-and-load)\n",
    "  - [Processing the time series](#Processing-the-time-series)\n",
    "    - [Percentage of clear pixels](#Percentage-of-clear-pixels)\n",
    "    - [Storing the result](#Storing-the-result)\n",
    "    - [Memory management](#Memory-management)\n",
    "  - [Displaying results](#Displaying-results)\n",
    "    - [Static plots](#Static-plots)\n",
    "    - [Animated output](#Animated-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook demonstrates how to create a time series of (mostly) cloud-free observations from Landsat 8, with the data displayed as an animation where each frame is annotated with its date.\n",
    "\n",
    "This notebook is adapted from an earlier [Digital Earth Australia](https://github.com/GeoscienceAustralia/dea-notebooks) example by Robbi Bishop-Taylor, and a [related notebook](https://github.com/GeoscienceAustralia/dea-notebooks/blob/develop/Frequently_used_code/Animated_timeseries.ipynb) can also be found in the current DEA notebook series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook setup\n",
    "\n",
    "In this section, we import the relevant Python modules and functions needed in the rest of this notebook. We also define some input variables for the subsequent analysis.\n",
    "\n",
    "## Importing modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### System\n",
    "import os, sys\n",
    "\n",
    "### Datacube \n",
    "import datacube\n",
    "from datacube import Datacube\n",
    "from datacube.utils import masking\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "\n",
    "### Data tools\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "### Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "# EASI tools\n",
    "import git\n",
    "import sys, os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "if repo.working_tree_dir not in sys.path: sys.path.append(repo.working_tree_dir + '/scripts')\n",
    "from easi_tools import EasiDefaults\n",
    "import notebook_utils\n",
    "easi = EasiDefaults()\n",
    "\n",
    "from app_utils import animated_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local dask cluster\n",
    "\n",
    "For local cluster options, see https://docs.dask.org/en/latest/setup/single-distributed.html\n",
    "\n",
    "The Dask Dashboard link shown after the following cell is a helpful resource to explore the activity and state of your dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster, client = notebook_utils.initialize_dask(workers=(1,2), use_gateway=False, wait=True)\n",
    "display(cluster if cluster else client)\n",
    "print(notebook_utils.localcluster_dashboard(client, server=easi.hub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### AWS configuration\n",
    "We will be using data in public requester-pays buckets, so the following configuration is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"This function obtains credentials for S3 access and passes them on to\n",
    "   processing threads, either local or on dask cluster.\n",
    "   Note that AWS credentials may need to be renewed between sessions or\n",
    "   after a period of time.\"\"\"\n",
    "\n",
    "from datacube.utils.aws import configure_s3_access\n",
    "configure_s3_access(aws_unsigned=False, requester_pays=True, client=client)\n",
    "\n",
    "# If not using a dask cluster then remove 'client':\n",
    "# configure_s3_access(aws_unsigned=False, requester_pays=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the analysis variables\n",
    "\n",
    "In the next cell, we set up the (user-defined) input variables for the current analysis: geographical extent, time period of interest, output resolution, and minimum proportion of cloud-free pixels used to filter the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Centre point of area to analyse, with a surrounding buffer (in metres) around it\n",
    "lat, lon, buffer_m = 36.364343, -114.379592, 8000   # Lake Mead, Nevada\n",
    "\n",
    "### Time window for which to return the Landsat data\n",
    "time_range = ('2017-01-01', '2023-04-01')\n",
    "\n",
    "### Output resolution (in meters)\n",
    "resolution = (-30, 30)\n",
    "\n",
    "### Minimum proportion of cloud-free pixels required to retain time slices\n",
    "# E.g. a proportion of 0.95 means that only observations with less than 5% cloud will be returned.\n",
    "landsat_cloud_free_prop = 0.85\n",
    "\n",
    "### Output animation file\n",
    "gif_out_file = './outputs/animated_Landsat_time_series.gif'   # results saved to the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading up the Landsat-8 time series\n",
    "\n",
    "## DataCube query\n",
    "\n",
    "Before loading up the data we're interested in, let's connect to the DataCube database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='Time series animation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in order to load data from the DataCube database, we will create a `query` (Python dictionary) containing the relevant analysis parameters. \n",
    "\n",
    "### Satellite product\n",
    "\n",
    "The Landsat-8 product we will use here is called `landsat8_c2l2_sr`. To review the available data coverage, please visit https://explorer.eail.easi-eo.solutions/products/landsat8_c2l2_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product = 'landsat8_c2l2_sr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate this product (without loading up any data at this stage) by listing its specific measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc.list_measurements().loc[product]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can work out the specific bands of interest (e.g. `red`, `green`, `blue`) as well as the name of the pixel quality band (`qa_pixel`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_bands = ['red', 'green', 'blue', 'qa_pixel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial extents\n",
    "\n",
    "When loading up the dataset, we can essentially request the data in any specific projection. Here, we will be using the dataset's native projection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set your own coordinates above, you might need to change the output CRS. There are various tools online to determine which UTM zone to set it manually or you can calculate it in python as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# product_crs = 'epsg:32611'   # WGS 84 / UTM zone 12N\n",
    "\n",
    "product_crs = f'epsg:{int(32700-np.round((45+lat)/90,0)*100+np.round((183+lon)/6,0))}'\n",
    "print(product_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now convert the centre point of our analysis window to that projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Define lon/lat centre point as Geometry object\n",
    "lon_lat_point = geometry.point(lon, lat, CRS('WGS84'))\n",
    "\n",
    "### Re-project centre point from lat/long to the selected projection\n",
    "x, y = lon_lat_point.to_crs(product_crs).points[0]\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use these projected coordinates as input to our query. But most importantly, we can also apply our buffer `buffer_m` around the re-projected centre point (`x`, `y`). Earlier, we defined the buffer value in metres &ndash; this requires the centre point to be in projected coordinates.\n",
    "\n",
    "### Data query and load\n",
    "\n",
    "We create the DataCube query by selecting a square window around the point of interest. We also select the time window of interest, as well as the desired satellite bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = { 'x': (x-buffer_m, x+buffer_m),\n",
    "          'y': (y-buffer_m, y+buffer_m),    \n",
    "          'time': time_range,\n",
    "          'measurements' : selected_bands,\n",
    "          'crs': product_crs,          # CRS of 'x','y' query parameters\n",
    "          'output_crs': product_crs,   # CRS of output data\n",
    "          'resolution': resolution,\n",
    "          'dask_chunks': {'time': 1}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Load data\n",
    "data = dc.load(product=product, group_by='solar_day', **query)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `data` object is an `Xarray` dataset with 147 time slices, and around 530 pixels in both the `x` and `y` dimensions. We can work out the size of this variable in memory, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"The size of 'data' in memory is {data.nbytes/(1024**2):.2f} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the time series\n",
    "\n",
    "In each time slice, we will calculate the proportion of clear pixels by summing the pixels that are not flagged as being problematic in the Landsat PQ layer. Here, only cloudy pixels, cloud-shadowed pixels, or pixels without valid data in every band are included in set of \"problematic\" pixels.\n",
    "\n",
    "## Percentage of clear pixels\n",
    "\n",
    "The \"cloud mask\" for this dataset is constructed using the pixel quality (PQ) data layer `qa_pixel` and a few quality flags to remove clouds and cloud shadows. We can can simply count the number of cloud free observations in each image, and divide it by the total number of pixels to give us a cloud-free percentage.\n",
    "\n",
    "To do this, we use the `xarray` function `xarray.sum()`, which sums up all the array values along the specified dimension(s). The array we'll apply this function to is filled with boolean values (`True` or `False`, essentially seen as 1 or 0 from the perspective of the `sum` function), and we will sum over both the `x` and `y` dimensions. This will thus give us the total number of cloud free pixels in each time slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "good_pixel_flags = {\n",
    "    'nodata': False,\n",
    "    'cloud': 'not_high_confidence', # 'not_high_confidence' means that it is unlikely that there is cloud. 'high_confidence' for cloud means that it is probably a cloudy pixel\n",
    "    'cloud_shadow': 'not_high_confidence' # 'not_high_confidence' means that it is unlikely that there is cloud shadow. 'high_confidence' for cloud shadow means that the pixel probably has cloud shadow\n",
    "}\n",
    "### Mask of clear pixels (for each time slices): True/1.0 if clear, False/0.0 if cloud-affected\n",
    "clear_mask = masking.make_mask(data.qa_pixel, **good_pixel_flags) \n",
    "\n",
    "### Cloud-free percentages\n",
    "_, x_len, y_len = clear_mask.shape\n",
    "cloud_free_percentage = clear_mask.sum(dim=['x','y']) / (x_len*y_len) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting object `cloud_free_percentage` is an array of percentages, one per time step. We can use the built-in `plot` function to display this time series as a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cloud_free_percentage.plot(aspect=3, size=4)\n",
    "plt.axhline(y=landsat_cloud_free_prop*100, color='r', linestyle=':')\n",
    "plt.text( x=cloud_free_percentage.time.values[0], y=landsat_cloud_free_prop*0.9*100, \n",
    "          s='landsat_cloud_free_prop', size='x-large', c='red')\n",
    "\n",
    "plt.xlabel('time')\n",
    "plt.ylabel(r'percentage')\n",
    "plt.title(\"Percentage of clear pixels through time\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the result\n",
    "\n",
    "The most useful place for the cloud percentage time series is with our original dataset (`data`). It corresponds to a single value per time slice, so let's add it back to `data` as a data variable, and give it a `time` coordinate. The timestamps are the same as the original observations, so we can construct the new data variable as a `DataArray` with the calculated percentage values, and a `time` coordinate copied from the original `data` Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['cloud_free_percentage'] = xr.DataArray(cloud_free_percentage, coords=[('time', data.time.data)])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a single `Xarray Dataset` containing all our Landsat measurements (bands) and the newly computed `cloud_free_percentage` variable, for every time slice.\n",
    "\n",
    "Now, we only want to retain the timeslices that meet our cloud-free percentage threshold. We can simply use `Xarray` indexing to do this, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Indices of time slices with (mostly) clear pixels\n",
    "selected_time_indices = ( data.cloud_free_percentage>=landsat_cloud_free_prop )\n",
    "\n",
    "### Select time slices from the data\n",
    "filtered_data = data.sel(time=selected_time_indices)\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the removal of time slices significantly affected by cloud and other issues, we now have a dataset with only 19 time slices (compared to the 68 time steps in the original dataset).\n",
    "\n",
    "## Memory management\n",
    "\n",
    "At this point we have several variables that we no longer need in our dataset, chewing up (some) memory. From this point on, we only need the \"filtered\" dataset, so let's set other variables to `None`, which will free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = None\n",
    "cloud_free_percentage = None\n",
    "cloud_mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can also remove the `pixel_qa` band from the `filtered_data` object &ndash; we used that band earlier to \"mask\" the time series, and thus we will not be needing it anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_data = filtered_data.drop_vars('qa_pixel')\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying results\n",
    "\n",
    "## Static plots\n",
    "\n",
    "We can plot the result as a series of images, one per time step. For plotting reasons, we need to set all the invalid data to `NaN` first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Set 'nodata' pixels to 'NaN' for all data variables\n",
    "filtered_data = masking.mask_invalid_data( filtered_data[['red', 'green', 'blue']] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data we want and the data is relatively small, we can choose to use `.compute()` to bring the results back from the local Dask cluster to local memory, which will make visualisation quicker.\n",
    "\n",
    "Note that the data below no longer shows each Data variable as a `dask.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_data = filtered_data.compute()\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select a few time indices from the time series, and plot the corresponding maps as RGB images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_time = filtered_data.sizes['time']\n",
    "time_ind = np.linspace(1, num_time, 16, dtype='int') - 1    # select a few time indices for plotting\n",
    "filtered_data.isel(time=time_ind).to_array().plot.imshow(robust=True, col='time', col_wrap=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated output\n",
    "\n",
    "Time series can also be visualised as animations, e.g. as animated stack of RGB data. So let's see how we can animate the current dataset. \n",
    "\n",
    "The code below calls upont the function `animated_timeseries()`, which can be found in the ancillary `./scripts/app_utils.py` Python file. This function is a modified version of an earlier [DEA](https://github.com/GeoscienceAustralia/dea-notebooks) function by R. Bishop-Taylor, S. Chua, and B. Dunn, which has been here simplified for demonstration purposes.\n",
    "\n",
    "From the help docstring, the `animated_timeseries()` function takes an `Xarray` time series as input, and renders the data as a three-band (true or false colour) animation, allowing changes in the landscape to be visualised over time. It also allows animations to be exported in the .mp4 (ideal for Twitter/social media), .wmv (ideal for Powerpoint) or .gif (ideal for all purposes, but can have large file sizes) formats. In addition, the animations can be customised to include titles and date annotations, or use specific combinations of input bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Create output directory if needed\n",
    "os.makedirs( os.path.dirname(gif_out_file), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### RGB animation of the Landsat time series\n",
    "animation = animated_timeseries( ds = filtered_data,\n",
    "                                 output_path = gif_out_file,\n",
    "                                 bands = ['red', 'green', 'blue'],\n",
    "                                 interval = 80,                        # milliseconds between each animation frame\n",
    "                                 width_pixels = 500,                   # output width in pixels for the resulting animation\n",
    "                                 title = 'Landsat 8 (low cloud)',\n",
    "                                 percentile_stretch = [0.01, 0.99] )   # percentage clip for RGB values\n",
    "plt.close()   # don't display the plot window used to generate the animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can now also display the resuting .gif animation in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(filename=gif_out_file, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further processing is likely in order to make this animation smoother and more refined. But from this output, we can already clearly observe some of the dynamics of land cover changes over time, as well as the rise and fall of the water level in the reservoir, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### End notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
