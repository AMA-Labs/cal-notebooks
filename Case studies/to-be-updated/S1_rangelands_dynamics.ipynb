{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rangelands dynamics with Sentinel-1\n",
    "\n",
    "This notebook gives an example of using Sentinel-1 analysis ready data (ARD) radar backscatter (Gamma nought) and dual-polarimetric decomposition (Alpha, Entropy and Anisotropy) for exploring annual rangeland dynamics.\n",
    "\n",
    "Sentinel-1 (in this case it is the dual-pol Radar Vegetation Index (RVI) and Entropy bands) and Sentinel-2 NDVI are compared for multitemporal trends due to their relationships with vegetation biomass. A grasslands mask (read in as a shape file) is then applied to extract only the areas of interest, before the monthly means are generated for the RVI and Entropy bands. These monthly means as well as the annual range in RVI and Entropy are then output as GeoTIFF files.\n",
    "\n",
    "The following steps are used:\n",
    "1.  Load Sentinel-1 radar backscatter data through the datacube API\n",
    "2.  Apply speckle filtering\n",
    "3.  Calculate the dual-pol Radar Vegetation Index (RVI)\n",
    "4.  Load Sentinel-1 dual-pol decomposition data\n",
    "5.  Load Sentinel-2 data and calculate NDVI\n",
    "6.  Interactively compare RVI, Entropy and NDVI time series\n",
    "7.  Show annual statistics\n",
    "8.  Generate monthly means\n",
    "9.  Save results to GeoTIFF files\n",
    "\n",
    "This notebook was written using the Virtual Desktop Infrastructure (VDI) on the National Computational Infrastructure (NCI), allowing access to the current Sentinel-1 and Digitial Earth Australia (DEA) ARD datasets. For this notebook to work you must first load the relevant modules:\n",
    "-  module use /g/data/v10/public/modules/modulefiles\n",
    "-  module load dea\n",
    "\n",
    "# 1. Load Sentinel-1 radar backscatter data through the datacube API\n",
    "\n",
    "Area of interest is in the Fitzroy Catchment of Western Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant modules for this Jupyter Notebook and start datacube\n",
    "import sys\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "from cartopy import crs\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_to_dict\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "#from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mplImage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='datacube')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.CRITICAL)\n",
    "for logger_name in ('boto', 'boto3', 'botocore', 's3transfer', 'rasterio', 'distributed'):\n",
    "    logging.getLogger(logger_name).setLevel(logging.CRITICAL)\n",
    "    \n",
    "dc = datacube.Datacube(env='datacube')#, config='radar.conf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask_kubernetes import KubeCluster\n",
    "from dask.distributed import Client,LocalCluster\n",
    "from dask.distributed import wait, progress\n",
    "cluster = None\n",
    "# Click on the 'Dashboard link' to monitor calculation progress\n",
    "cluster = LocalCluster()\n",
    "cluster.scale_up(4)\n",
    "#cluster.adapt(minimum=1, maximum=3, scale_factor=2, startup_cost=\"10s\", wait_count=5, target_duration=\"10s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributed\n",
    "client = distributed.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select area of interest for year of interest \n",
    ">  Currently only use 2017 or 2018 for full years <br>\n",
    ">  Note that the area of interest must be small to avoid memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, WMSLayer, LayersControl\n",
    "\n",
    "m = Map(center=(-35.283, 149.128), zoom=12, layout=dict(height='600px'))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Year = '2019'\n",
    "myloc = 'ACT'\n",
    "# Set up centre of area to analyse, and a buffer in metres around this centrepoint\n",
    "lat, lon, buffer_m = -35.283, 149.128, (10*1000)\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {\n",
    "'y': (y - buffer_m, y + buffer_m),\n",
    "'x': (x - buffer_m, x + buffer_m),\n",
    "'time': (Year + '-05-01', Year + '-07-31'),\n",
    "'crs': 'EPSG:3577',\n",
    "'output_crs': 'EPSG: 3577',\n",
    "'resolution': (20, 20)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read radar backscatter, clean and smooth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=dc.load(product='s1_intensity_scene', group_by='solar_day', **query, dask_chunks={'time': 1})\n",
    "\n",
    "# Identify images that have minimum nulls\n",
    "# Uses code from https://github.com/fangfy/dea-projects/blob/master/water_interoperability/sentinel1_load_and_classify_nci.ipynb\n",
    "\n",
    "total_px=len(bs.x)*len(bs.y)\n",
    "valid=bs.where(bs.vv!=0).where(bs.vh!=0).count(dim=('x','y'))\n",
    "\n",
    "good=(valid.vh/total_px)>0.5\n",
    "\n",
    "bs_good = bs.sel(time=good)\n",
    "# replace 0 with nan\n",
    "bs_clean = bs_good.where(bs_good!=0)\n",
    "\n",
    "# Adapted from https://stackoverflow.com/questions/39785970/speckle-lee-filter-in-python\n",
    "from scipy.ndimage.filters import uniform_filter\n",
    "from scipy.ndimage.measurements import variance\n",
    "\n",
    "def lee_filter(da, size):\n",
    "    img = da.values\n",
    "    img_mean = uniform_filter(img, (size, size))\n",
    "    img_sqr_mean = uniform_filter(img**2, (size, size))\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "    return img_output\n",
    "\n",
    "# save the nodata mask\n",
    "nodata_mask = bs_clean.isnull().to_array().any(axis=0)\n",
    "# Convert backscatter nans to 0 for lee filter\n",
    "bs_good_zerofilled = bs_good.where(~bs_good.isnull(), 0)\n",
    "\n",
    "# Apply speckle filter\n",
    "smoothed_vv=bs_good_zerofilled.vv.groupby('time').apply(lee_filter, size=7)\n",
    "smoothed_vh=bs_good_zerofilled.vh.groupby('time').apply(lee_filter, size=7)\n",
    "\n",
    "# Create smoothed dataset with Nans and assign attributes\n",
    "smoothed=smoothed_vv.to_dataset(name='vv')\n",
    "smoothed['vh']=smoothed_vh\n",
    "smoothed=smoothed.where(~nodata_mask)\n",
    "\n",
    "# Remove unused data\n",
    "bs_attrs = bs.attrs\n",
    "smoothed = smoothed.assign_attrs(bs_attrs)\n",
    "#smoothed = smoothed.persist()\n",
    "\n",
    "del bs, bs_good, bs_clean\n",
    "print('Backscatter data for ', myloc,': ',smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View smoothed images (VV or VH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv.extension('bokeh')\n",
    "dataset = smoothed.load()\n",
    "dataset_vv = gv.Dataset(dataset.drop('vh'), ['x', 'y', 'time'], 'vv', crs=crs.epsg(3577))\n",
    "dataset_vv = dataset_vv.redim.range(vv=(0.0,0.2))\n",
    "dataset_vh = gv.Dataset(sataset.drop('vv'), ['x', 'y', 'time'], 'vh', crs=crs.epsg(3577))\n",
    "dataset_vh = dataset_vh.redim.range(vh=(0.0,0.08))\n",
    "\n",
    "images = dataset_vv.to(gv.Image).opts(cmap='viridis', colorbar=True, projection=crs.epsg(3577), width=492, height=500) + dataset_vh.to(gv.Image).opts(colorbar=True, projection=crs.epsg(3577), width=492, height=500)\n",
    "\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View smoothed data (Note - you might need to edit 'figsize' to improve appearance)\n",
    "ntimes=len(smoothed.time.values)\n",
    "\n",
    "@interact(mypol=['VV','VH'])\n",
    "\n",
    "def plot(mypol='VV'):\n",
    "    if mypol == 'VV': smoothed.vv.isel(time=slice(0,ntimes,2)).plot(col='time',col_wrap=4, vmin=0, vmax=0.2, figsize=(40,20));\n",
    "    if mypol == 'VH': smoothed.vh.isel(time=slice(0,ntimes,2)).plot(col='time',col_wrap=4, vmin=0, vmax=0.08, figsize=(40,20));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and view modified dual-pol Radar Vegetation Index images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View RVI data\n",
    "\n",
    "smoothed['RVI'] = 4*smoothed.vh/(smoothed.vv + smoothed.vh)\n",
    "\n",
    "ntimes=len(smoothed.time.values)\n",
    "smoothed.RVI.isel(time=slice(0,ntimes,2)).plot(col='time',col_wrap=4, vmin=0.2, vmax=1.5, figsize=(20,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in dual-pol decomposition data\n",
    "## This results in Alpha, Anisotropy and Entropy bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp=dc.load(product='s1_decomposition_scene', group_by='solar_day', **query, dask_chunks={'time': 1} )\n",
    "\n",
    "dp = dp.where(dp.entropy!=0)\n",
    "dp = dp.persist()\n",
    "print('Dual polarimetric decomposition data for ', myloc,': ',dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Alpha or Entropy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntimes=len(dp.time.values)\n",
    "\n",
    "# View entropy (values range from 0-1.0) data \n",
    "# (can also change to look at 'anisotropy' (values ~0-1.0))\n",
    "\n",
    "@interact(mypol=['Alpha','Entropy'])\n",
    "\n",
    "def plot(mypol='Alpha'):\n",
    "    if mypol == 'Alpha': dp.alpha.isel(time=slice(0,ntimes,2)).plot(col='time',col_wrap=4, vmin=40, vmax=80.0, figsize=(20,10));\n",
    "    if mypol == 'Entropy': dp.entropy.isel(time=slice(0,ntimes,2)).plot(col='time',col_wrap=4, vmin=0, vmax=1.0, figsize=(20,10));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sentinel-2 to extract NDVI time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Year = '2019'\n",
    "myloc = 'ACT'\n",
    "# Set up centre of area to analyse, and a buffer in metres around this centrepoint\n",
    "lat, lon, buffer_m = -35.283, 149.128, (20*1000)\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {\n",
    "'y': (y - buffer_m, y + buffer_m),\n",
    "'x': (x - buffer_m, x + buffer_m),\n",
    "'time': (Year + '-05-16', Year + '-10-30'),\n",
    "'crs': 'EPSG:3577',\n",
    "'output_crs': 'EPSG: 3577',\n",
    "'resolution': (-20, 20)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_S2 = datacube.Datacube(env='datacube', app='dc-S2-extract')\n",
    "\n",
    "# Read S2 green, nir and cloud mask bands for S2a and S2b\n",
    "bands_of_interest = ['B04_20m', 'B03_20m','B02_20m', 'B8A_20m', 'SCL_20m']\n",
    "s2_orig = dc_S2.load(product = 's2_l2a_scene', group_by='solar_day', measurements = bands_of_interest, **query, dask_chunks={'time': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s2_orig.rename({'B04_20m': 'red',\n",
    "                     'B03_20m': 'green',\n",
    "                     'B02_20m': 'blue',\n",
    "                     'B8A_20m': 'nir',\n",
    "                     'SCL_20m': 'SCL'})\n",
    "del s2_orig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube.storage import masking\n",
    "# Set all nodata pixels to `NaN`:\n",
    "s2 = masking.mask_invalid_data(s2)\n",
    "\n",
    "# Identify images that have minimum nulls and remove them\n",
    "# Uses code from https://github.com/fangfy/dea-projects/blob/master/water_interoperability/sentinel1_load_and_classify_nci.ipynb\n",
    "\n",
    "total_px=len(s2.x)*len(s2.y)\n",
    "valid=s2.where(s2.red!=0).where(s2.SCL<=7).where(s2.SCL>=2).where(s2.SCL!=3).count(dim=('x','y'))\n",
    "\n",
    "good=(valid.red/total_px)>0.5\n",
    "\n",
    "s2_good = s2.sel(time=good)\n",
    "\n",
    "# replace 0 with nan\n",
    "s2_good_clean = s2_good.where(s2_good!=0)\n",
    "del s2_good\n",
    "\n",
    "s2 = s2_good_clean\n",
    "del s2_good_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s2.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = s2[['red', 'green', 'blue']].to_array().compute()\n",
    "percentile_stretch=(0.02, 0.95)\n",
    "vmin, vmax = image_array.quantile(percentile_stretch).values\n",
    "# Show the image\n",
    "image_array.plot.imshow(col='time', col_wrap=4,vmin=vmin, vmax=vmax,robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s2_good_temp = s2_good.where(s2_good.fmask<2) # to remove cloud/shadow/nulls based on fmask\n",
    "#s2_good_clean = s2_good_temp.where(s2_good_temp.fmask!=0)\n",
    "\n",
    "#s2_good_clean = s2_good_clean.assign_attrs(bs_attrs)\n",
    "\n",
    "#del s2_good\n",
    "\n",
    "# Create NDVI band\n",
    "s2['ndvi']=(s2.nir.astype(float) - s2.red.astype(float))/(s2.nir.astype(float) + s2.red.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any remaining erroneous values (where NDVI <-1.0 and NDVI > 1.0)\n",
    "s2['ndvi'] = s2.ndvi.where(s2.ndvi > -1.0).where(s2.ndvi < 1.0)\n",
    "print('Sentinel-2 data for ', myloc,': ',s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View NDVI images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View S2 NDVI time series\n",
    "ntimes=len(s2.ndvi.time.values)\n",
    "s2.ndvi.plot(cmap='viridis', col='time', col_wrap=2,figsize=(20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select area of interest to interactively to look at RVI, Entropy and NDVI time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is disabled as it won't work with jupyterlabs at the moment\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "#def onclick(event):\n",
    "#    global pixelx, pixely\n",
    "#    x, y = int(event.xdata), int(event.ydata)\n",
    "#    image_coords = smoothed.affine * (x, y)\n",
    "#    pixelx = x \n",
    "#    pixely = y \n",
    "#    w.value = 'pixelx : {}, pixely : {}'.format(pixelx, pixely)\n",
    "#    print('data values:',x,y)\n",
    "#\n",
    "#print('\\033[1m' + 'Click on the pixel to view its time series below' + '\\033[0m')\n",
    "#\n",
    "#fig = plt.figure(figsize = (5,5))\n",
    "#plt.imshow(np.flip(smoothed.vv.mean(dim='time')), interpolation = 'nearest', clim=(0,0.1), extent=[smoothed.coords['x'].min(), smoothed.coords['x'].max(), smoothed.coords['y'].min(), smoothed.coords['y'].max()]); \n",
    "#\n",
    "#w = widgets.HTML(\"Click on the pixel to view its time series below\")\n",
    "#\n",
    "#cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "#display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot monthly time series based on selected pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelx, pixely = 1555010,-3956214\n",
    "# plot RVI, Entropy and NDVI through time based on the x,y point selected\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, interactive\n",
    "warnings.filterwarnings('ignore', module='datacube')\n",
    "\n",
    "# expand selected x,y pixel to make a square area of interest\n",
    "xp, yp=slice(pixelx-100,pixelx+100), slice(pixely-100,pixely+100)\n",
    "\n",
    "@interact(myname=['monthly RVI','monthly Entropy'])\n",
    "\n",
    "def plot(myname='monthly RVI'):\n",
    "    global myplot, myimage, mytitle\n",
    "    if myname == 'monthly RVI': \n",
    "        (smoothed.sel(x=xp,y=yp).RVI.groupby('time.month').mean()).plot(color='r', figsize=(10,3));\n",
    "        myplot = (smoothed.sel(x=xp,y=yp).RVI.groupby('time').mean())\n",
    "        myimage = smoothed.RVI\n",
    "        mytitle = 'RVI (all images)'\n",
    "    if myname == 'monthly Entropy': \n",
    "        (dp.sel(x=xp,y=yp).entropy.groupby('time.month').mean()).plot(color='b', figsize=(10,3));\n",
    "        myplot = (dp.sel(x=xp,y=yp).entropy.groupby('time').mean())\n",
    "        myimage = dp.entropy\n",
    "        mytitle = 'Entropy (all images)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s2.sel(x=xp,y=yp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View time series of selected timeseries (RVI or Entropy) based on all scenes\n",
    "### Select a point to view scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on a point in the time-series from RVI or Entropy (selected above) to display the image\n",
    "\n",
    "#Use this plot to visualise a time series and select the image that corresponds with a point in the time series\n",
    "def callback(event):\n",
    "    global time_int, devent\n",
    "    devent = event\n",
    "    time_int = event.xdata\n",
    "    w.value = 'time_int: {}'.format(time_int)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "\n",
    "fig.canvas.mpl_connect('button_press_event', callback)\n",
    "plt.show()\n",
    "display(w)\n",
    "myplot.plot(linestyle= '--', c= 'b', marker = '8', mec = 'b', mfc ='r');\n",
    "plt.grid()\n",
    "plt.title(mytitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot image from selected time slice\n",
    "\n",
    "time_slice = matplotlib.dates.num2date(time_int).date()\n",
    "myimage.sel(time=time_slice, method='nearest').plot(vmin=0.2, vmax=1.5, figsize=(8,6));\n",
    "plt.title(time_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Annual Statistics\n",
    "## Annual difference in RVI, Entropy and NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate annual range (max minus min) for radar backscatter RVI, dul-pol decomposition Entropy, and NDVI\n",
    "from matplotlib import pyplot as plt\n",
    "pylab.rcParams['font.size']=7\n",
    "\n",
    "smoothed['RVI_range'] = smoothed.RVI.max(dim='time') - smoothed.RVI.min(dim='time')\n",
    "dp['entropy_range'] = dp.entropy.max(dim='time') - dp.entropy.min(dim='time')\n",
    "s2_good_clean['ndvi_range'] = s2_good_clean.ndvi.max(dim='time') - s2_good_clean.ndvi.min(dim='time')\n",
    "\n",
    "fix, axes = plt.subplots(ncols=3, figsize=(9.5,2.2))\n",
    "\n",
    "smoothed.RVI_range.plot(vmin=0.2,vmax=1.2, ax=axes[0]);\n",
    "dp.entropy_range.plot(vmin=0.1,vmax=0.5, ax=axes[1]);\n",
    "s2_good_clean.ndvi_range.plot(vmin=0.0,vmax=0.5, ax=axes[2]);\n",
    "plt.tight_layout()\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual RVI, Entropy and NDVI mean, min, max and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show RVI, Entropy and NDVI mean, min, max and stand deviation for current year\n",
    "\n",
    "fig, axes = plt.subplots(ncols=4, nrows=3, figsize=(10,5))\n",
    "\n",
    "smoothed.RVI.mean(dim='time').plot(vmin=0.0,vmax=1.0, ax=axes[0,0]);\n",
    "smoothed.RVI.min(dim='time').plot(vmin=0.0,vmax=1.0, ax=axes[0,1]);\n",
    "smoothed.RVI.max(dim='time').plot(vmin=0.0,vmax=2.0, ax=axes[0,2]);\n",
    "smoothed.RVI.std(dim='time').plot(vmin=0.0,vmax=0.5, ax=axes[0,3]);\n",
    "dp.entropy.mean(dim='time').plot(vmin=0.2,vmax=0.8, ax=axes[1,0]);\n",
    "dp.entropy.min(dim='time').plot(vmin=0.0,vmax=0.8, ax=axes[1,1]);\n",
    "dp.entropy.max(dim='time').plot(vmin=0.0,vmax=1.2, ax=axes[1,2]);\n",
    "dp.entropy.std(dim='time').plot(vmin=0.05,vmax=0.2, ax=axes[1,3]);\n",
    "s2_good_clean.ndvi.mean(dim='time').plot(vmin=0.1,vmax=0.5, ax=axes[2,0]);\n",
    "s2_good_clean.ndvi.min(dim='time').plot(vmin=0.0,vmax=0.5, ax=axes[2,1]);\n",
    "s2_good_clean.ndvi.max(dim='time').plot(vmin=0.0,vmax=1.0, ax=axes[2,2]);\n",
    "s2_good_clean.ndvi.std(dim='time').plot(vmin=0.0,vmax=0.2, ax=axes[2,3]);\n",
    "\n",
    "axes[0,0].set_title('Mean')\n",
    "axes[0,1].set_title('Min')\n",
    "axes[0,2].set_title('Max')\n",
    "axes[0,3].set_title('Std')\n",
    "plt.tight_layout()\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and view monthly mean RVI images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot RVI, Entropy and NDVI through time based on the x,y point selected\n",
    "#%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "time = pd.to_datetime(['2017-01-01','2017-02-01','2017-03-01','2017-04-01','2017-05-01','2017-06-01',\n",
    "                             '2017-07-01','2017-08-01','2017-09-01','2017-10-01','2017-11-01','2017-12-01'])\n",
    "\n",
    "monthly_mean_RVI=xr.Dataset({'Jan':(smoothed.RVI.sel(time=slice('2017-01-01', '2017-01-31')).mean(dim='time')), 'time': time})\n",
    "monthly_mean_RVI['Feb']=smoothed.RVI.sel(time=slice('2017-02-01', '2017-02-28')).mean(dim='time')\n",
    "monthly_mean_RVI['Mar']=smoothed.RVI.sel(time=slice('2017-03-01', '2017-03-31')).mean(dim='time')\n",
    "monthly_mean_RVI['Apr']=smoothed.RVI.sel(time=slice('2017-04-01', '2017-04-30')).mean(dim='time')\n",
    "monthly_mean_RVI['May']=smoothed.RVI.sel(time=slice('2017-05-01', '2017-05-31')).mean(dim='time')\n",
    "monthly_mean_RVI['Jun']=smoothed.RVI.sel(time=slice('2017-06-01', '2017-06-30')).mean(dim='time')\n",
    "monthly_mean_RVI['Jul']=smoothed.RVI.sel(time=slice('2017-07-01', '2017-07-31')).mean(dim='time')\n",
    "monthly_mean_RVI['Aug']=smoothed.RVI.sel(time=slice('2017-08-01', '2017-08-31')).mean(dim='time')\n",
    "monthly_mean_RVI['Sep']=smoothed.RVI.sel(time=slice('2017-09-01', '2017-09-30')).mean(dim='time')\n",
    "monthly_mean_RVI['Oct']=smoothed.RVI.sel(time=slice('2017-10-01', '2017-10-31')).mean(dim='time')\n",
    "monthly_mean_RVI['Nov']=smoothed.RVI.sel(time=slice('2017-11-01', '2017-11-30')).mean(dim='time')\n",
    "monthly_mean_RVI['Dec']=smoothed.RVI.sel(time=slice('2017-12-01', '2017-12-31')).mean(dim='time')\n",
    "\n",
    "#monthly_mean_RVI = monthly_mean_RVI.assign_attrs(bs_attrs)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "@interact(month=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
    "\n",
    "def plot(month='Jan'):\n",
    "    if month == 'Jan': plt.imshow(np.flip(monthly_mean_RVI.Jan), clim=(0,1.0))\n",
    "    if month == 'Feb': plt.imshow(np.flip(monthly_mean_RVI.Feb), clim=(0,1.0))\n",
    "    if month == 'Mar': plt.imshow(np.flip(monthly_mean_RVI.Mar), clim=(0,1.0))\n",
    "    if month == 'Apr': plt.imshow(np.flip(monthly_mean_RVI.Apr), clim=(0,1.0))\n",
    "    if month == 'May': plt.imshow(np.flip(monthly_mean_RVI.May), clim=(0,1.0))\n",
    "    if month == 'Jun': plt.imshow(np.flip(monthly_mean_RVI.Jun), clim=(0,1.0))\n",
    "    if month == 'Jul': plt.imshow(np.flip(monthly_mean_RVI.Jul), clim=(0,1.0))\n",
    "    if month == 'Aug': plt.imshow(np.flip(monthly_mean_RVI.Aug), clim=(0,1.0))\n",
    "    if month == 'Sep': plt.imshow(np.flip(monthly_mean_RVI.Sep), clim=(0,1.0))\n",
    "    if month == 'Oct': plt.imshow(np.flip(monthly_mean_RVI.Oct), clim=(0,1.0))\n",
    "    if month == 'Nov': plt.imshow(np.flip(monthly_mean_RVI.Nov), clim=(0,1.0))\n",
    "    if month == 'Dec': plt.imshow(np.flip(monthly_mean_RVI.Dec), clim=(0,1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and view monthly mean Entropy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a monthly mean\n",
    "# plot RVI, Entropy and NDVI through time based on the x,y point selected\n",
    "#%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "time = pd.to_datetime(['2017-01-01','2017-02-01','2017-03-01','2017-04-01','2017-05-01','2017-06-01',\n",
    "                             '2017-07-01','2017-08-01','2017-09-01','2017-10-01','2017-11-01','2017-12-01'])\n",
    "\n",
    "monthly_mean_entropy=xr.Dataset({'Jan':(dp.entropy.sel(time=slice('2017-01-01', '2017-01-31')).mean(dim='time')), 'time': time})\n",
    "monthly_mean_entropy['Feb']=dp.entropy.sel(time=slice('2017-02-01', '2017-02-28')).mean(dim='time')\n",
    "monthly_mean_entropy['Mar']=dp.entropy.sel(time=slice('2017-03-01', '2017-03-31')).mean(dim='time')\n",
    "monthly_mean_entropy['Apr']=dp.entropy.sel(time=slice('2017-04-01', '2017-04-30')).mean(dim='time')\n",
    "monthly_mean_entropy['May']=dp.entropy.sel(time=slice('2017-05-01', '2017-05-31')).mean(dim='time')\n",
    "monthly_mean_entropy['Jun']=dp.entropy.sel(time=slice('2017-06-01', '2017-06-30')).mean(dim='time')\n",
    "monthly_mean_entropy['Jul']=dp.entropy.sel(time=slice('2017-07-01', '2017-07-31')).mean(dim='time')\n",
    "monthly_mean_entropy['Aug']=dp.entropy.sel(time=slice('2017-08-01', '2017-08-31')).mean(dim='time')\n",
    "monthly_mean_entropy['Sep']=dp.entropy.sel(time=slice('2017-09-01', '2017-09-30')).mean(dim='time')\n",
    "monthly_mean_entropy['Oct']=dp.entropy.sel(time=slice('2017-10-01', '2017-10-31')).mean(dim='time')\n",
    "monthly_mean_entropy['Nov']=dp.entropy.sel(time=slice('2017-11-01', '2017-11-30')).mean(dim='time')\n",
    "monthly_mean_entropy['Dec']=dp.entropy.sel(time=slice('2017-12-01', '2017-12-31')).mean(dim='time')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "@interact(month=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
    "\n",
    "def plot(month='Jan'):\n",
    "    if month == 'Jan': plt.imshow(np.flip(monthly_mean_entropy.Jan), clim=(0,1.0))\n",
    "    if month == 'Feb': plt.imshow(np.flip(monthly_mean_entropy.Feb), clim=(0,1.0))\n",
    "    if month == 'Mar': plt.imshow(np.flip(monthly_mean_entropy.Mar), clim=(0,1.0))\n",
    "    if month == 'Apr': plt.imshow(np.flip(monthly_mean_entropy.Apr), clim=(0,1.0))\n",
    "    if month == 'May': plt.imshow(np.flip(monthly_mean_entropy.May), clim=(0,1.0))\n",
    "    if month == 'Jun': plt.imshow(np.flip(monthly_mean_entropy.Jun), clim=(0,1.0))\n",
    "    if month == 'Jul': plt.imshow(np.flip(monthly_mean_entropy.Jul), clim=(0,1.0))\n",
    "    if month == 'Aug': plt.imshow(np.flip(monthly_mean_entropy.Aug), clim=(0,1.0))\n",
    "    if month == 'Sep': plt.imshow(np.flip(monthly_mean_entropy.Sep), clim=(0,1.0))\n",
    "    if month == 'Oct': plt.imshow(np.flip(monthly_mean_entropy.Oct), clim=(0,1.0))\n",
    "    if month == 'Nov': plt.imshow(np.flip(monthly_mean_entropy.Nov), clim=(0,1.0))\n",
    "    if month == 'Dec': plt.imshow(np.flip(monthly_mean_entropy.Dec), clim=(0,1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write output scenes to GeoTIFFs\n",
    "## Output annual range and monthly mean RVI and Entropy images to GeoTIFF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(myoutputdir=['/g/data/qd04/Cate/TempProcessing/','/g/data/qd04/Cate/'])\n",
    "\n",
    "def plot(myoutputdir='/g/data/qd04/Cate/TempProcessing/'):\n",
    "    global mydir\n",
    "    if myoutputdir == '/g/data/qd04/Cate/TempProcessing/': mydir = '/g/data/qd04/Cate/TempProcessing/'\n",
    "    if myoutputdir == '/g/data/qd04/Cate/':mydir = '/g/data/qd04/Cate/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(myoutputfiles=['output files','stop'])\n",
    "\n",
    "def plot(myoutputfiles='stop'):\n",
    "    if myoutputfiles == 'output files': \n",
    "        print('Writing GeoTIFF files')\n",
    "    \n",
    "        # write output images (in Albers) to GeoTIFF\n",
    "\n",
    "        import ogr, gdal, osr\n",
    "\n",
    "        #mydir = '/g/data/qd04/Cate/TempProcessing/'\n",
    "\n",
    "        # define coordinates for Albers equal area (3577)\n",
    "        xcoords = smoothed.isel(time=1).vv.indexes['x']\n",
    "        ycoords = smoothed.isel(time=1).vv.indexes['y']\n",
    "        yt,xt = smoothed.isel(time=1).vv.shape\n",
    "        MaxValX = xcoords.shape\n",
    "        MaxValY = ycoords.shape\n",
    "\n",
    "        # set geotransform and output projection\n",
    "        xres = 25 \n",
    "        yres = 25 \n",
    "        geotransform = (xcoords[MaxValX[0]-1]-(xres*0.5), xres, 0, ycoords[MaxValY[0]-1]+(yres*0.5), 0, -yres) # offset by half the pixel size since it needs to be top-left pixel coord\n",
    "        srs = osr.SpatialReference() \n",
    "        srs.ImportFromEPSG(3577)\n",
    "\n",
    "        # loop through monthly images\n",
    "        months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "\n",
    "        # Edit output filenames (filename_RVI and filename_entropy) and directory as required\n",
    "        for month in months:\n",
    "\n",
    "            filename_RVI = 'S1_RVI_3577_'+ month +'_'+ Year +'.tif' \n",
    "            S1_ds = gdal.GetDriverByName('GTiff').Create(mydir+filename_RVI, \n",
    "                                                           xt, yt, 1, gdal.GDT_Float32)\n",
    "            S1_ds.SetGeoTransform(geotransform) # specify coordinates\n",
    "            S1_ds.SetProjection(srs.ExportToWkt()) # export coords to file\n",
    "            S1_ds.GetRasterBand(1).WriteArray(np.flip(monthly_mean_RVI[month]).data) # write band to raster\n",
    "            S1_ds.FlushCache()  # write to file\n",
    "            S1_ds = None # save and close\n",
    "\n",
    "            filename_entropy = 'S1_Entropy_3577_'+ month + '_'+ Year + '.tif' \n",
    "            S1_ds = gdal.GetDriverByName('GTiff').Create(mydir+filename_entropy, \n",
    "                                                           xt, yt, 1, gdal.GDT_Float32)\n",
    "            S1_ds.SetGeoTransform(geotransform) # specify coordinates\n",
    "            S1_ds.SetProjection(srs.ExportToWkt()) # export coords to file\n",
    "            S1_ds.GetRasterBand(1).WriteArray(np.flip(monthly_mean_entropy[month]).data) # write band to raster\n",
    "            S1_ds.FlushCache()  # write to file\n",
    "            S1_ds = None # save and close\n",
    "\n",
    "        # Output annual RVI and Entropy range images\n",
    "        filename_RVI = 'S1_RVI_3577_Annual_Range_' + Year +'.tif' \n",
    "        S1_ds = gdal.GetDriverByName('GTiff').Create(mydir+filename_RVI, \n",
    "                                                           xt, yt, 1, gdal.GDT_Float32)\n",
    "        S1_ds.SetGeoTransform(geotransform) # specify coordinates\n",
    "        S1_ds.SetProjection(srs.ExportToWkt()) # export coords to file\n",
    "        S1_ds.GetRasterBand(1).WriteArray(np.flip(smoothed.RVI_range).data) # write band to raster\n",
    "        S1_ds.FlushCache()  # write to file\n",
    "        S1_ds = None # save and close\n",
    "\n",
    "        # Output annual RVI and Entropy range images\n",
    "        filename_entropy = 'S1_Entropy_3577_Annual_Range_' + Year + '.tif' \n",
    "        S1_ds = gdal.GetDriverByName('GTiff').Create(mydir+filename_entropy, \n",
    "                                                           xt, yt, 1, gdal.GDT_Float32)\n",
    "        S1_ds.SetGeoTransform(geotransform) # specify coordinates\n",
    "        S1_ds.SetProjection(srs.ExportToWkt()) # export coords to file\n",
    "        S1_ds.GetRasterBand(1).WriteArray(np.flip(dp.entropy_range).data) # write band to raster\n",
    "        S1_ds.FlushCache()  # write to file\n",
    "        S1_ds = None # save and close\n",
    "\n",
    "    if myoutputfiles == 'stop': print('Finished without writing GeoTIFF files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "cluster = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
