{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Using a shapefile to mask raster data</u> <img align=\"right\" src=\"../resources/csiro_easi_logo.png\">\n",
    "\n",
    "**Contents**\n",
    "\n",
    "  - [Overview](#Overview)\n",
    "  - [Notebook setup](#Notebook-setup)\n",
    "  - [Loading up the Sentinel-2 data](#Loading-up-the-Sentinel-2-data)\n",
    "    - [DataCube query](#DataCube-query)\n",
    "    - [Data load](#Data-load)\n",
    "    - [Cleaning up the data](#Cleaning-up-the-data)\n",
    "    - [Time series display](#Time-series-display)\n",
    "  - [Masking the raster data with a shapefile](#Masking-the-raster-data-with-a-shapefile)\n",
    "    - [Loading up the shapefile](#Loading-up-the-shapefile)\n",
    "    - [Creating a raster mask](#Creating-a-raster-mask)\n",
    "    - [Applying the mask to the data time series](#Applying-the-mask-to-the-data-time-series)\n",
    "    \n",
    "\n",
    "# Overview\n",
    "\n",
    "This notebook demonstrates how to use a shapefile to mask raster data extracted from the EAIL. For demonstration purposes, the notebook makes use of Sentinel-2 data and uses an existing shapefile containing the polygon(s) to apply as a mask. This shapefile is available in the `ancillary_data` folder in this CSIRO EASI case studies repository.\n",
    "\n",
    "This notebook is adapted from a [Digital Earth Australia](https://github.com/GeoscienceAustralia/dea-notebooks) example by Claire Krause.\n",
    "\n",
    "**A note regarding compatibility**\n",
    "\n",
    "As stated above, this notebook makes use of a specific dataset of remote sensing data, and is applied over a given region of interest. The code below will thus not run properly if the EASI deployment used to run this notebook does not also contain a similar dataset.\n",
    "\n",
    "Users can nonetheless investigate the outputs provided in this demonstration notebook, and also potentially modify certain variables in the code below to allow the notebook to run with a different EASI deployment (e.g. different remote sensing data, region of interest, etc.)\n",
    "\n",
    "\n",
    "# Notebook setup\n",
    "\n",
    "Here, we import the relevant Python modules and functions needed in the rest of this notebook. Subsequently, we open a connection to the EASI datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### System\n",
    "import os, sys\n",
    "\n",
    "### Datacube \n",
    "from datacube import Datacube\n",
    "from datacube.utils import masking\n",
    "from odc.algo import enum_to_bool\n",
    "\n",
    "### Data tools\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import geopandas as gpd\n",
    "\n",
    "### Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### EASI tools\n",
    "sys.path.append(os.path.expanduser('../scripts'))\n",
    "import notebook_utils\n",
    "from app_utils import display_map\n",
    "from easi_tools import EasiDefaults\n",
    "easi = EasiDefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T05:35:24.010171Z",
     "start_time": "2018-08-29T05:35:17.970519Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc = Datacube(app='Raster mask from shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Start a local dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster, client = notebook_utils.initialize_dask(use_gateway=False)\n",
    "display(cluster if cluster else client)\n",
    "print(notebook_utils.localcluster_dashboard(client, server=easi.hub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### AWS configuration\n",
    "We will be using data in public requester-pays buckets, so the following configuration is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"This function obtains credentials for S3 access and passes them on to\n",
    "   processing threads, either local or on dask cluster.\n",
    "   Note that AWS credentials may need to be renewed between sessions or\n",
    "   after a period of time.\"\"\"\n",
    "\n",
    "from datacube.utils.aws import configure_s3_access\n",
    "configure_s3_access(aws_unsigned=False, requester_pays=True, client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading up the Sentinel-2 data\n",
    "\n",
    "## DataCube query\n",
    "\n",
    "In this notebook, we're interested in a dataset of Sentinel-2 imagery. We set up the corresponding spatial extent in the `query` dicitionary below, together with a (small) temporal window using the defaults for this deployment. \n",
    "\n",
    "We will use the Sentinel-2 data for false-colour plots of the SWIR, NIR and green bands (in the corresponding RGB channels). We thus add these bands to the list of `measurements` to load up, together with the `SCL` band of pixel quality data, which we will use to remove bad-quality pixels.\n",
    "\n",
    "We will also plot the dataset in its original (native) projection, so in the code below, we use the function `mostcommon_crs()` to work out the dataset's native coordinate reference system (CRS) and set the query parameter `output_crs` to that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This configuration is read from the defaults for this system. \n",
    "# Examples are provided in a commented line to show how to set these manually.\n",
    "\n",
    "study_area_lat = easi.latitude\n",
    "# study_area_lat = (39.2, 39.3)\n",
    "\n",
    "study_area_lon = easi.longitude\n",
    "# study_area_lon = (-76.7, -76.6)\n",
    "\n",
    "product = easi.product('sentinel-2')\n",
    "# product = 'landsat8_c2l2_sr'\n",
    "\n",
    "set_time = easi.time\n",
    "# set_time = ('2020-08-01', '2020-12-01')\n",
    "\n",
    "# set_crs = easi.crs('sentinel-2')\n",
    "# set_crs = 'EPSG:32618'\n",
    "\n",
    "set_resolution = easi.resolution('sentinel-2')\n",
    "# set_resolution = (-30, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = { 'product': product,\n",
    "          'lat': study_area_lat,\n",
    "          'lon': study_area_lon,\n",
    "          'measurements': ['green', 'nir_1', 'swir_2', 'SCL'],\n",
    "          'time': set_time,\n",
    "          'group_by': 'solar_day' }        # scene ordering\n",
    "\n",
    "### Dataset's native projection\n",
    "native_crs = notebook_utils.mostcommon_crs(dc, query)\n",
    "print(f\"The dataset's native CRS is: {native_crs}\")\n",
    "\n",
    "query.update({ 'output_crs': native_crs,     # EPSG code\n",
    "               'resolution': set_resolution, # target resolution\n",
    "               'dask_chunks': {'time': 1}})  # Dask config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dc.load(**query)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <p><strong>NOTE:</strong> Because our dataset is here relatively small, we did not make use of `Dask` (distributed `Xarray` processing on compute clusters). The data will therefore take a minute or two to load.</p>\n",
    "    <p>As usual, we can check the size of the data object we just loaded up into the system memory &ndash; using a very large dataset (without `Dask`) could lead to technical and/or computational issues in the code further below in this notebook.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The size of 'data' in memory is {data.nbytes/(1024**2):.2f} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the data\n",
    "\n",
    "Once loaded, we typically want to (pre-)process the data to remove pixels flagged as `nodata`, as well as those pixels affected by specific data issues such as cloud, cloud shadow, saturation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Valid mask (i.e. not 'nodata'), for each data layer\n",
    "valid_mask = masking.valid_data_mask(data)\n",
    "\n",
    "### Mask of valid pixels\n",
    "good_pixel_flags = {'water', 'vegetation', 'bare soils', 'unclassified', 'dark area pixels'}   # pixels to retain (i.e. remove pixels flagged as 'nodata', 'cloud', 'shadow', 'snow')\n",
    "good_pixel_mask = enum_to_bool(data['SCL'], good_pixel_flags)  # -> DataArray\n",
    "\n",
    "### Scaling factors for Sentinel-2 data (GA S2 products)\n",
    "scale = 0.0001  # divide by 10000\n",
    "offset = 0.0\n",
    "\n",
    "### Apply valid mask, good pixel mask, and scaling to each layer\n",
    "data['swir_2'] = data['swir_2'].where(valid_mask['swir_2'] & good_pixel_mask) * scale + offset\n",
    "data['nir_1'] = data['nir_1'].where(valid_mask['nir_1'] & good_pixel_mask) * scale + offset\n",
    "data['green'] = data['green'].where(valid_mask['green'] & good_pixel_mask) * scale + offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series display\n",
    "\n",
    "For demonstration purposes, we can also select a small number of time slice to display from the original time series of 11 images. Below, we identify the four time indices that contain the largest numbers of valid pixels, and extract these time slices from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_clear_pix = good_pixel_mask.sum(['x','y'])   # number of valid pixels, in each time slice\n",
    "pix_thr = np.sort( num_clear_pix )[-4]     # sort the time series and select a threshold (4th largest number)\n",
    "clearest_ind = (num_clear_pix>=pix_thr)    # indices of time slices with nr of pixels above threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Extract time slices from the time series\n",
    "data = data.isel(time=clearest_ind)\n",
    "data = data.compute() # Use the Dask compute() function to bring the data to local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Plot the resulting time series (false-colour display)\n",
    "image_array = data[['swir_2', 'nir_1', 'green']].to_array()\n",
    "image_array.plot.imshow(robust=True, col='time', col_wrap=2, size=6, aspect=data.x.shape[0]/data.y.shape[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking the raster data with a shapefile\n",
    "\n",
    "## Loading up the shapefile\n",
    "\n",
    "For this example, we are using a training shapefile which has been provided in the default EASI configuration. This shapefile is also provided in the `ancillary_data` folder in this repository. You can upload and use your own data in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T05:35:29.491091Z",
     "start_time": "2018-08-29T05:35:29.486957Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shape_file = easi.training_shapefile\n",
    "print(f'Using shapefile {shape_file}')\n",
    "### Load the shapefile\n",
    "shp = gpd.read_file(shape_file)\n",
    "display(shp)\n",
    "shp.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can see here that the vector data within that shapefile is in a WGS84 geographic datum (`EPSG:4326`), which is different from that of our main Sentinel-2 dataset. For compatibility, we can here re-project the shapefile data to the CRS of the Sentinel-2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Reproject\n",
    "shp = shp.to_crs(native_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Plot\n",
    "shp.boundary.plot(figsize=(8,8))\n",
    "plt.xlabel(\"x [metre]\"); plt.ylabel(\"y [metre]\")\n",
    "plt.title(\"Training shapefile\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a raster mask\n",
    "\n",
    "We can now create a raster mask from the vector data. The code below iterates over the polygons in the shapefile (in case multiple polygons are available), setting the raster mask values to `1` for all the pixels corresponding to the footprint of each polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T05:35:29.830439Z",
     "start_time": "2018-08-29T05:35:29.673975Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = rasterio.features.rasterize( ((feature['geometry'], 1) for feature in shp.iterfeatures()),\n",
    "                                    out_shape = (data.dims['y'],data.dims['x']),\n",
    "                                    transform = data.affine )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T05:35:29.838006Z",
     "start_time": "2018-08-29T05:35:29.833034Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Convert the mask (numpy array) to an Xarray DataArray\n",
    "mask = xr.DataArray(mask, coords=(data.y, data.x))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T05:35:29.938180Z",
     "start_time": "2018-08-29T05:35:29.840231Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask.plot(figsize=(8,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the mask to the data time series\n",
    "\n",
    "Finally, we can use the mask we just created, apply it to the time series of Sentinel-2 data, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Masking\n",
    "data = data.where(mask)\n",
    "\n",
    "### Plotting\n",
    "image_array = data[['swir_2', 'nir_1', 'green']].to_array()\n",
    "image_array.plot.imshow(robust=True, col='time', col_wrap=2, size=6, aspect=data.x.shape[0]/data.y.shape[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T05:35:30.027851Z",
     "start_time": "2018-08-29T05:35:29.970230Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### End notebook"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
