{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data <img align=\"right\" src=\"../resources/csiro_easi_logo.png\">\n",
    "\n",
    "This notebook will show you how to load data from the Open Data Cube (ODC) using the Python API. \n",
    "\n",
    "#### Index\n",
    "- [Setup](#Setup)\n",
    "- [Create a connection to the datacube](#Create-a-connection-to-the-datacube)\n",
    "- [The datacube load() function](#The-datacube-load()-function)\n",
    "- [Xarray Dataset and DataArray objects](#Xarray-Dataset-and-DataArray-objects)\n",
    "- [Specify measurements](#Specify-measurements)\n",
    "   - [Measurement aliases](#Measurement-aliases)\n",
    "- [Specify time range](#Specify-time-range)\n",
    "- [Specify spatial extent](#Specify-spatial-extent)\n",
    "- [Specify output CRS and resolution](#Specify-output-CRS-and-resolution)\n",
    "   - [No default CRS](#No-default-CRS)\n",
    "   - [Reproject](#Reproject)\n",
    "   - [Resample](#Resample)\n",
    "   - [Align](#Align)\n",
    "- [Reuse load parameters](#Reuse-load-parameters)\n",
    "   - [Load parameters that match an existing dataset](#Load-parameters-that-match-an-existing-dataset)\n",
    "- [No available data](#No-available-data)\n",
    "- [Load large datasets with Dask](#Load-large-datasets-with-Dask)\n",
    "   - [Data type](#Data-type)\n",
    "   - [Rechunking](#Rechunking)\n",
    "   - [Dask persist and compute](#Dask-persist-and-compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "### Dask\n",
    "Create or reuse a dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional EASI tools\n",
    "import sys, os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "sys.path.append(os.path.expanduser('../scripts'))\n",
    "from easi_tools import EasiDefaults\n",
    "import notebook_utils\n",
    "easi = EasiDefaults()\n",
    "cluster, client = notebook_utils.initialize_dask(use_gateway=False, wait=True)\n",
    "display(cluster if cluster else client)\n",
    "print(notebook_utils.localcluster_dashboard(client, server=easi.hub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS configuration\n",
    "To use data in public requester-pays buckets, run the following code (once per dask cluster):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datacube.utils.aws import configure_s3_access\n",
    "configure_s3_access(aws_unsigned=False, requester_pays=True, client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a connection to the datacube\n",
    "\n",
    "The `Datacube` object created below (which we have named `dc`) will be used to interact with the ODC database.\n",
    "\n",
    "The EASI Jupyter environment is configured with default credentials to access the ODC database so there is no need to pass connection details to the `Datacube()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datacube\n",
    "dc = datacube.Datacube()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The datacube `load()` function\n",
    "\n",
    "The ODC documentation describes each of the options available for loading data: https://datacube-core.readthedocs.io/en/latest/dev/api/generate/datacube.Datacube.load.html\n",
    "\n",
    "At a minimum specify the `product name` and `location extents` of the data you wish to load.\n",
    "\n",
    "Below we define `latitude`/`longitude` extents for a small area and then load the [Landsat 8 surface reflectance](https://explorer.csiro.easi-eo.solutions/landsat8_c2l2_sr) product.\n",
    "\n",
    "> The examples below use **Dask** via the `dask_chunks` option. If using a dask local cluster (`gateway_cluster=False`) or no dask, the notebook will load the data into the memory of your Jupyter Lab instance and if you exceed the memory specifications your Jupyter Lab instance will crash.\n",
    "\n",
    "> Dask loading is quick because no data is loaded yet; only the data load request has been added to the Dask scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This configuration is read from the defaults for this system. \n",
    "# Examples are provided below each line to show how to set these manually.\n",
    "\n",
    "study_area_lat = easi.latitude\n",
    "# study_area_lat = (39.2, 39.3)\n",
    "\n",
    "study_area_lon = easi.longitude\n",
    "# study_area_lon = (-76.7, -76.6)\n",
    "\n",
    "product = easi.product('landsat')\n",
    "# product = 'landsat8_c2l2_sr'\n",
    "\n",
    "set_time = easi.time\n",
    "# set_time = ('2020-08-01', '2020-12-01')\n",
    "\n",
    "set_crs = easi.crs('landsat')\n",
    "# set_crs = 'EPSG:32618'\n",
    "\n",
    "set_resolution = easi.resolution('landsat')\n",
    "# set_resolution = (-30, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dc.load(\n",
    "    product = product, \n",
    "    x = study_area_lon,\n",
    "    y = study_area_lat,\n",
    "    output_crs=set_crs,\n",
    "    resolution=set_resolution,\n",
    "    dask_chunks = {\"time\":1}\n",
    ")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xarray `Dataset` and `DataArray` objects\n",
    "\n",
    "The load function returns an `xarray.Dataset` object as a three-dimensional gridded dataset containing the requested product measurements.  \n",
    "\n",
    "Calling `display(data)` in a Jupyter notebook (as in the cell above) prints a helpful overview of the dataset, showing the following:\n",
    "\n",
    "- `Dimensions` - The labels and sizes of each of the datasets dimensions.\n",
    "- `Coordinates` - The corresponding values along each dimension. \n",
    "- `Data variables` - An `xarray.DataArray` for each of the product measurements.\n",
    "- `Attributes` - Additional metadata including the coordinate reference system (`crs`)\n",
    "\n",
    "Click on the <html><i class=\"fa fa-file-text-o\" style=\"font-size:18px;color:gray\"></i></html> icon to the right of one of the `Coordinates` or `Data variables` above to see it's attributes (e.g. `units`, `resolution`, and `nodata` value where applicable). Click on the <html><i class=\"fa fa-database\" style=\"font-size:18px;color:gray\"></i></html> icon to reveal the underlying data array.\n",
    "\n",
    "For more details on the `xarray.Dataset` structure, see http://xarray.pydata.org/en/stable/data-structures.html#dataset.\n",
    "\n",
    "Accessing a single variable by name returns an `xarray.DataArray` object. The actual data array can then be accessed with:\n",
    "- `xarray.DataArray.data` - The array’s data as a dask or numpy array\n",
    "- `xarray.DataArray.values` - The array’s data as a numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "# Return a DataArray\n",
    "display(HTML('<h3>A <em>DataArray</em> object:</h3>'))\n",
    "blue = data[\"blue\"]  # or data.blue\n",
    "display(blue)\n",
    "\n",
    "# Return a Dataset\n",
    "display(HTML('<h3>A <em>Dataset</em> object:</h3>'))\n",
    "blue_ds = data[[\"blue\"]]  # List of measurements\n",
    "display(blue_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify measurements\n",
    "\n",
    "By default, `dc.load()` will return all measurements available for the specified product. Use the `measurements` parameter to specify a subset measurement(s).\n",
    "\n",
    "> Using dask it can be as easy to request all measurements (default) and then operate on only the variables required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "measurements = (\"red\", \"green\", \"blue\")\n",
    "data = dc.load(\n",
    "    product = product,\n",
    "    measurements=measurements,\n",
    "    x = study_area_lon,\n",
    "    y = study_area_lat,\n",
    "    output_crs=set_crs,\n",
    "    resolution=set_resolution,\n",
    "    dask_chunks = {\"time\":1}\n",
    ")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement aliases\n",
    "\n",
    "Product measurements often have aliases that can also be used when loading. We can see the aliases by using the `Datacube.list_measurements()` function.\n",
    "\n",
    "For example, lets look at the `landsat8_c2l2_sr` product.\n",
    "\n",
    "Load the `\"qa_pixel\"` measurement using the `\"pixel_quality\"` alias instead. The resulting xarray variable will be named by the alias in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_meas = dc.list_measurements()\n",
    "ls8_meas = all_meas.loc[product]\n",
    "disp_columns = [\"name\", \"aliases\"]\n",
    "ls8_meas[disp_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dc.load(\n",
    "    product = product,\n",
    "    measurements = [\"pixel_quality\"],\n",
    "    x = study_area_lon,\n",
    "    y = study_area_lat,\n",
    "    output_crs=set_crs,\n",
    "    resolution=set_resolution,\n",
    "    dask_chunks = {\"time\":1}\n",
    ")\n",
    "display(data)\n",
    "\n",
    "# Note that the Xarray variable is named as the alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify time range\n",
    "\n",
    "For the query above all available timesteps for the dataset were returned. To specify a time range (or a single time slice) you can pass a `time` option to `load()`, as below.\n",
    "\n",
    "Time formats are parsed according to standard python date time parsing (https://dateutil.readthedocs.io/en/stable/parser.html).\n",
    "\n",
    "The `time` option accepts a single value or a range.\n",
    "- `time = ('2013-06-22 16:44.33', '2015-01-10')`\n",
    "- `time = '2016-03'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time = ('2020-02', '2020-04')\n",
    "data = dc.load(\n",
    "    product = product,\n",
    "    measurements = measurements,\n",
    "    x = study_area_lon,\n",
    "    y = study_area_lat,\n",
    "    time = time,\n",
    "    output_crs=set_crs,\n",
    "    resolution=set_resolution,\n",
    "    dask_chunks = {\"time\":1}\n",
    ")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify spatial extent\n",
    "\n",
    "The following keys specify the ***search*** spatial extent in `datacube.load()`.\n",
    "\n",
    "- `x`, `lon`, `long`, `longitude` - Single or a range of values\n",
    "- `y`, `lat`, `latitude` - Single or a range of values\n",
    "- `crs` - Coordinate reference system (CRS) in which to interpret the `x` and `y` values|\n",
    "\n",
    "In the above examples we have used `x = longitude range` and `y = latitude range`. \n",
    "\n",
    "These are combined into a Polygon (or Line or Point) with the attached CRS. The default CRS is `EPSG:4326`. Use a different CRS to, for example, specify UTM coordinates.\n",
    "\n",
    "The examples above request the `x`/`y` coordinates as longitude and latitude values (i.e., `EPSG:4326`). Here we load the same data as above by defining the search coordinates in a different CRS (`EPSG:32719`), and show that they are equal.\n",
    "\n",
    "> The `landsat8_c2l2_sr` native CRS varies, but in this part of the United States, it is WGS 84 / UTM zone 18N (EPSG:32618) and the `x`/`y` coordinates are defined in metres with 30m resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the UTM coords from data\n",
    "\n",
    "xs = data.x.data\n",
    "ys = data.y.data\n",
    "x = (xs.min(), xs.max())\n",
    "y = (ys.min(), ys.max())\n",
    "\n",
    "print(f'UTM x-coords: {x}')\n",
    "print(f'UTM y-coords: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_xy = dc.load(\n",
    "    product = product,\n",
    "    x = x,\n",
    "    y = y,\n",
    "    crs = set_crs,\n",
    "    measurements = measurements,\n",
    "    time = time,\n",
    "    output_crs = set_crs,\n",
    "    resolution = set_resolution,\n",
    "    dask_chunks = {\"time\":1}\n",
    ")\n",
    "print(f\"Datasets are equal: {data.equals(data_xy)}\")\n",
    "display(data_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify output CRS and resolution \n",
    "\n",
    "The following keys specify the ***result*** CRS, resolution and resampling in datacube.load()\n",
    "- `output_crs` - Target CRS (default is the native CRS, if defined)\n",
    "- `resolution` - Target pixel resolution _(y-pixel, x-pixel)_ (default is the native resolution)\n",
    "- `resampling` - Resampling method when resolution is different to source resolution (default is nearest neighbor)\n",
    "- `align` - Shift coordinate values to align with pixel edges _(y-pixel, x-pixel)_\n",
    "\n",
    "The next sections show examples of each.\n",
    "\n",
    "### No default CRS\n",
    "\n",
    "Some products do not have a default CRS, in which case you will need to provide `output_crs` and `resolution` parameters when calling `load()`, otherwise you will recieve the following error:\n",
    "\n",
    "```\n",
    "ValueError: Product has no default CRS. Must specify 'output_crs' and 'resolution'\n",
    "```\n",
    "\n",
    "> This can be the case for products in a UTM projection where the spatial extent crosses multiple UTM zones or CRSs.\n",
    "\n",
    "### Reproject\n",
    "\n",
    "As an example, load our product into the WGS84 (EPSG:4326) CRS.\n",
    "\n",
    "Note that:\n",
    "- The spatial dimensions of the result are now labelled `longitude` and `latitude`, because the result is in a geographic coordinate system.\n",
    "- The `resolution` is provided in `output_crs` units.\n",
    "- The `resolution` includes the direction of the coordinates in the result, indicated by a positive or negative number. Positive resolution values result in coordinates in the range `minimum..maximum`. A negative resolution will reverse the coordinate range to be `maximum..minimum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dc.load(\n",
    "    product = product,\n",
    "    x = study_area_lon,\n",
    "    y = study_area_lat,\n",
    "    time = time,\n",
    "    output_crs = \"EPSG:4326\",\n",
    "    resolution = (-0.01, 0.01),  # Approximately 1 km\n",
    "    dask_chunks = {\"time\":1}\n",
    ")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample\n",
    "\n",
    "The `resolution` parameter essentially defines a resampling operation. The `resampling` parameter can be used to specify which method to use. Default is `resampling=\"nearest\"`.\n",
    "\n",
    "See https://datacube-core.readthedocs.io/en/latest/api/indexed-data/generate/datacube.Datacube.load.html for the full range of options, including allowing different resampling methods for different measurements (with a dict).\n",
    "\n",
    "Here we downsample the product from the native 30 m to 100 m resolution as the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dc.load(\n",
    "    product = product,\n",
    "    x = study_area_lon,\n",
    "    y = study_area_lat,\n",
    "    time = time,\n",
    "    output_crs = \"EPSG:3857\",\n",
    "    resolution = (-100, 100),\n",
    "    resampling = \"average\",\n",
    "    dask_chunks = {\"time\":1}\n",
    ")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align\n",
    "\n",
    "The `align` parameter shifts the coordinates to align with the pixel boundary. It is applied as an offset after the rounding of grid coordinates to the resolution. Values are in the range `0 <= align <= absolute(resolution)`.\n",
    "\n",
    "> While most scenarios should be handled correctly there a few places in the GDAL, Rasterio, Datacube stack that assume the top-left corner is the origin of the array. For most products, pixel boundary grid alignment will have been defined and checked in a preprocess stage but in case its not we have these options to adjust the grids in datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This example demonstrates the use of the `align` parameter but it is not necessarily an appropriate operation for this product\n",
    "# TODO: Find an example where `align` is required\n",
    "\n",
    "data = dc.load(\n",
    "    product = product,\n",
    "    x = study_area_lon,\n",
    "    y = study_area_lat,\n",
    "    time = time,\n",
    "    output_crs = \"EPSG:3857\",\n",
    "    resolution = (-100, 100),\n",
    "    align = (25, 25),\n",
    "    dask_chunks = {\"time\":1}\n",
    ")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse load parameters\n",
    "\n",
    "For ease of use, it is common to store the load parameters in a Python dictionary, which can be passed to the `load()` function using Python's keyword expansion operator `**`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"product\": product,\n",
    "    \"latitude\": study_area_lat,\n",
    "    \"longitude\": study_area_lon,\n",
    "    \"time\": (\"2013\", \"2015\"),\n",
    "    \"output_crs\": \"EPSG:4326\",\n",
    "    \"resolution\": (-0.01, 0.01),\n",
    "    \"dask_chunks\": {\"time\": 1}\n",
    "}\n",
    "data = dc.load(**query)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters that match an existing dataset \n",
    "\n",
    "Use the `like` parameter to load a new product to with the same extent, crs, and resolution as an existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data2 = dc.load(\n",
    "    product = \"landsat7_c2l2_sr\",\n",
    "    like = data,\n",
    "    dask_chunks = {\"time\": 1}\n",
    ")\n",
    "display(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No available data\n",
    "\n",
    "For some product/location combinations there may be no data available, or it may not be indexed in the datacube yet. In this case an empty dataset will be returned.\n",
    "\n",
    "You can view available products along with the data coverage at the EAIL Datacube Explorer: https://explorer.eail.easi-eo.solutions/products/landsat8_c2l2_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data3 = dc.load(\n",
    "    product = product,\n",
    "    x = (39.2, 39.3), \n",
    "    y = (-73.5, -73.4), # Note that the latitude has changed to be outside the available data area \n",
    "    dask_chunks = {\"time\": 1}\n",
    ")\n",
    "display(data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load large datasets with Dask\n",
    "\n",
    "It is possible to load datasets with and without Dask. The examples above have used Dask, but they are also small enough queries that fit within our environment's memory. If we look at the size of our `xarray.Dataset` in MB, we see that it is fairly small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_mb = data.nbytes / (1024 ** 2) \n",
    "print(f\"Dataset size: {data_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python Dask library allows us to load a dataset much larger than a single environment's memory by distributing the data across its dask \"workers\". Each worker is a dedicated compute node. Dask datasets are processed \"lazily\", which means data is only retrieved from storage and processed when it is needed.\n",
    "\n",
    "Datacube and xarray work well with Dask. Use the `dask_chunks` parameter to `load()` to specify the size of dask chunks (across dimensions). Data are not loaded and processed until an output is requested, for example: convert to a numpy array, create a plot, or write data to a file.\n",
    "\n",
    "**See the [Dask appendix notebook](A2%20-%20Dask.ipynb) for getting started and more information. See also the Dask arrays introduction https://docs.dask.org/en/latest/array.html.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets increase the extent of our query and use the `dask_chunks` parameter. Setting the chunks for the `x` and `y` dimensions to `2048` (and using the default chunk size of `1` for `time`) means that behind the scenes our data will be split into many `1 x 2048 x 2048` pixel arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latitude=easi.latitude_big\n",
    "longitude=easi.longitude_big\n",
    "chunks = {\"x\": 2048, \"y\": 2048}  # \"x\" and \"y\" match the dimension names from the datacube.load() result, so they may depend on `output_crs`\n",
    "big_data = dc.load(\n",
    "    product = product,\n",
    "    output_crs = \"EPSG:3857\",\n",
    "    resolution = (-300, 300),\n",
    "    x = longitude,\n",
    "    y = latitude,\n",
    "    group_by = 'solar_day',\n",
    "    dask_chunks = chunks,\n",
    ")\n",
    "display(big_data)  # This is a \"lazy\" request so its fast to show the structure but no data has been loaded yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the <html><i class=\"fa fa-database\" style=\"font-size:18px;color:gray\"></i></html> icon for one of the measurements shows the details of the `dask.Array`. Each variable is much larger than the previous example and has many more chunks. When using data of this size, extra thought needs to be put regarding how to manage memory on your Jupyter node or in your Dask cluster. You aren't going to be able to load this dataset into RAM and there could be significant overhead managing so many calculation tasks. With a good understanding of Dask, it is possible to process very large quantities of data.\n",
    "\n",
    "> Dask strategies (in [Dask appendix notebook](A2%20-%20Dask.ipynb)) suggests the chunk size should be no more than 100 MB.\n",
    "\n",
    "We can see from the `Dimensions` of the dataset above that it is significantly larger than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get size of `data` in bytes and covert to GB\n",
    "big_data_gb = big_data.nbytes / (1024 ** 3) \n",
    "print(f\"Dataset size: {big_data_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type\n",
    "\n",
    "In the example above our data are `uint16`, and chunks of `(1, 2048, 2028)` are `8 MB` each. Data processing, such as scaling and and calculating indices, will result in `float64` data, which will increase the chunk memory size.\n",
    "\n",
    "### Rechunking\n",
    "\n",
    "Datacube loads data in spatial layers and places these into an xarray `time, y, x` cube. If further processing (valid data, masking, scaling etc) are reasonably per pixel operations then dask chunks can remain as are (spatially-orientated). If doing time series processing then consider rechunking the xarray dask dataset on the time dimension.\n",
    "\n",
    "**Be aware that rechunking can result in very high memory usage when it comes to the point of running the calculations.**\n",
    "\n",
    "Dask chunk size of `-1` indicates we want all data along that dimension chunked together. Below we have reduced the `longitude`/`latitude` chunks sizes to even out the memory required for each chunk.\n",
    "\n",
    "> Click the <html><i class=\"fa fa-database\" style=\"font-size:18px;color:gray\"></i></html> icon to the right of any of the data variables to view the new chunking arrangement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rechunked = big_data.chunk({'time':-1, 'x':512, 'y':512})  # Example only\n",
    "display(rechunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask persist and compute\n",
    "\n",
    "The Dask `persist() and compute()` functions control how dask manages its chunks. See https://distributed.dask.org/en/latest/memory.html for a full discussion and additional options.\n",
    "\n",
    "> Use `persist()` to maintain the data on the cluster and use `compute()` to bring a small result back to the notebook environment's memory. **These should be used sparingly and with good knowledge of Dask operations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <p>Please review the other Dask notebooks in this repository to learn more about how to use Dask, particularly when using large datasets.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***More examples to come...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
