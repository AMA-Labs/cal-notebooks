{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Dask with Dask Gateway <img align=\"right\" src=\"../resources/csiro_easi_logo.png\">\n",
    " \n",
    "Index\n",
    "- [Background](#Background)\n",
    "- [Dask Concepts](#Dask-Concepts)\n",
    "- [Local Dask Cluster](#Local-dask-cluster)\n",
    "- [Dask Gateway](#Dask-Gateway)\n",
    "   - [Create cluster](#Create-cluster)\n",
    "   - [Scale the cluster](#Scale-the-cluster)\n",
    "   - [Connect to the cluster](#Connect-to-the-cluster)\n",
    "   - [Cluster status](#Cluster-status)\n",
    "   - [Shutdown the cluster](#Shutdown-the-cluster)\n",
    "- [EASI Utility Functions](#EASI-utility-functions)\n",
    "   - [Initialize a local cluster](#Initialize-a-local-cluster)\n",
    "   - [Initialize a remote cluster](#Initialize-a-remote-cluster)\n",
    "   - [Initialize a remote cluster with adaptive scaling](#Initialize-a-remote-cluster-with-adaptive-scaling)\n",
    "- [Dask strategies](#Dask-strategies)\n",
    "   - [Change the dask chunk size](#Change-the-dask-chunk-size)\n",
    "   - [Chunking examples](#Chunking-examples)\n",
    "   - [GridWorkflow](#GridWorkflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <p>This notebook is under development</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Dask is a python library for distributed (parallel) processing. We encourage users to create their own Dask cluster for their analysis. Dask allows us to work on very large array or datacube objects that would otherwise not fit in the memory of any single compute node.\n",
    "\n",
    "Why dask https://docs.dask.org/en/latest/why.html?\n",
    "\n",
    "> Python has grown to become a dominant language in data analytics and general programming. This is fueled both by computational libraries like `Numpy`, `Pandas`, and `Scikit-Learn` and by a wealth of libraries for visualization, interactive notebooks, collaboration, and so forth. However, these packages were not designed to scale beyond a single machine. Dask was developed to scale these packages and the surrounding ecosystem. Dask provides ways to scale Pandas, Scikit-Learn, and Numpy workflows more natively, with minimal rewriting. It integrates well with these tools so that it copies most of their API and uses their data structures internally.\n",
    "\n",
    "Dask works well with `Xarray`, which are the data objects returned from the `datacube`. There are many tutorials regarding the use of `Xarray`. See:\n",
    "- https://examples.dask.org/xarray.html\n",
    "- https://xarray.pydata.org/en/stable/dask.html\n",
    "\n",
    "[Holoviews](https://holoviews.org) is a visualisation package that works directly with dask as well. It will efficiently access only the pixels it requires for a visualisation from a dask data object distributed across your dask cluster. See the [Visualisation notebook](05%20-%20Visualising%20Data.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <strong>Note:</strong> In addition to this notebook, please refer to the Dask Tutorials in the <strong>Dask</strong> folder in the root of this repository, which provide more information about how and why to use Dask. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Concepts\n",
    "\n",
    "We use the `Dask.distributed` library. See [https://distributed.dask.org/en/latest](https://distributed.dask.org/en/latest).\n",
    "\n",
    "Dask operates in a \"lazy\" compute manner, which means it will only do processing work when it needs to and otherwise will store a list of operations until they are required. Dask figures out how to break up large computations and distribute the tasks to the dask cluster **workers**.\n",
    "\n",
    "The **dask scheduler** maintains the full list of tasks. For large or complex computations it can take a noticable amount of time for the dask schedule graph to be created or updated. A key part of learning to use dask effectively is to learn how to manage - and troubleshoot - the size of the schedule graph (see [Dask strategies](#Dask-strategies)).\n",
    "\n",
    "The **dask cluster** is the scheduler plus the group of compute **workers** that process the tasks in a distributed or parallel manner. The number of workers is initialised when the cluster is started. We encourage users to create *adaptive* clusters that add more workers when required (processing tasks) and reduces the number of workers when not required (thinking time).\n",
    "\n",
    "The size of eash dask worker in EASI is set to: `cores = 8, memory = 32 GB` (2x large). The operating system on a worker takes up some of the available memory and it is best leave some \"headroom\" for the processing. Therefore estimate that about `20 GB` is available per node.\n",
    "\n",
    "The **dask client** is the interface to the dask scheduler. The client can be used to submit tasks to the scheduler, and influence or retrieve information from the scheduler. In practice the submitting of tasks is usually handled by the `Xarray` or `Pandas` libraries. See https://distributed.dask.org/en/latest/client.html.\n",
    "\n",
    "Useful client functions that are used regularly:\n",
    "- `client.compute()` - run the tasks in the scheduler and return the result to the Jupyter notebook. ***Warning:*** if the data result is too large for the Jupyter notebook memory then it will crash.\n",
    "- `client.persist()` - run the tasks in the scheduler but leave the data on the dask workers.\n",
    "- `client.restart()` - restart the cluster and reset the scheduler.\n",
    "\n",
    "The **dask dashboard** (`client.dashboard_link`) is a helpful resource to explore the activity and state of your dask cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local dask cluster\n",
    "\n",
    "A local dask cluster runs entirely on the Jupyter notebook machine. It is useful for processing small data volumes (less than the Jupyter notebook memory) as it will utilise dask distribution of tasks across the available machine's processors. See https://docs.dask.org/en/latest/setup/single-distributed.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__Tip:__ This notebook starts by showing you how to start up a local cluster manually. Note that there are also some utility functions to make this easier. This is demonstrated at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, re\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "sys.path.append(os.path.expanduser('../scripts'))\n",
    "from easi_tools import EasiDefaults\n",
    "import notebook_utils\n",
    "easi = EasiDefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Connect to an existing LocalCluster if available\n",
    "# The default port for the scheduler is 8786\n",
    "# The default port for the dashboard is 8787, but you will get a different port number if a cluster already exists\n",
    "try:\n",
    "    # This creates a new Client connection to an existing Dask scheduler if one exists.\n",
    "    # There is no practical way to get the LocalCluster object from the existing scheduler,\n",
    "    # although the scheduler details can be accessed with `client.scheduler`.\n",
    "    # The LocalCluster object is only available from the notebook that created it.\n",
    "    # Restart the kernel or `client.close();cluster.close()` in each notebook that\n",
    "    # created one to remove existing LocalClusters.\n",
    "    client = Client('localhost:8786', timeout='2s')\n",
    "    cluster = client.cluster  # None\n",
    "except:\n",
    "    cluster = LocalCluster(scheduler_port=8786)\n",
    "    client = Client(cluster)\n",
    "\n",
    "display(cluster if cluster else client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LocalCluster dashboard link needs to be rewritten to use the JupyterLab proxy. We have put this code in a `notebook_utils` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(notebook_utils.localcluster_dashboard(client, server=easi.hub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Gateway\n",
    "\n",
    "**[Dask Gateway](https://gateway.dask.org/)** is a centralised system that helps to manage dask functionality and associated workers. We use Dask Gateway to create clusters and then connect notebooks to the cluster to provide scalable computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initiliaze the Gateway client\n",
    "from dask.distributed import Client\n",
    "from dask_gateway import Gateway\n",
    "\n",
    "gateway = Gateway()\n",
    "gateway.cluster_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster options shows how each worker will be configured. There are a limited set of options and valid values that can be configured.\n",
    "\n",
    "The [dask usage guide](https://gateway.dask.org/usage.html) describes how to edit and set the cluster options. We use the default configuration here.\n",
    "\n",
    "> __Only__ use a worker size that you require for your work. Excessive worker sizes will increase costs and reduce resources for other users.\n",
    "\n",
    "### Create cluster\n",
    "\n",
    "Create the cluster with default options if it doesn't already exist. If a cluster exists in your namespace, the code below will connect to that cluster. List the available clusters with `gateway.list_clusters()`.\n",
    "\n",
    "***This may take a few minutes if new dask nodes need to be created in AWS (if its quick then existing nodes are being reused).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusters = gateway.list_clusters()\n",
    "if not clusters:\n",
    "    print('Creating new cluster. Please wait for this to finish.')\n",
    "    cluster = gateway.new_cluster()\n",
    "else:\n",
    "    print(f'An existing cluster was found. Connecting to: {clusters[0].name}')\n",
    "    cluster=gateway.connect(clusters[0].name)\n",
    "display(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the cluster\n",
    "\n",
    "Use the GatewayCluster widget to adjust the cluster size. Alternatively use the cluster API methods.\n",
    "\n",
    "For many tasks `1 or 2 workers` will be sufficient, although for larger areas or more complex tasks 5 to 10 workers may be used. If you are new to Dask, start with one worker and then scale your cluster if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_number_of_workers = 1\n",
    "max_number_of_workers = 4\n",
    "\n",
    "# Static scaling\n",
    "cluster.scale(min_number_of_workers)\n",
    "\n",
    "# Adaptive scaling\n",
    "cluster.adapt(minimum=min_number_of_workers, maximum=max_number_of_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the cluster\n",
    "To connect to your cluster and start doing work, use the `get_client()` method. This step will wait until the workers are ready.\n",
    "\n",
    ">__NOTE:__ This will take a few minutes before your cluster will be ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = cluster.get_client()\n",
    "client.wait_for_workers(n_workers=min_number_of_workers)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster status\n",
    "To check the status of your cluster, call the `cluster` object. Any cluster widget shown on your page will automatically update to show the new workers.\n",
    "\n",
    "Click the `\"Dashboard\"` link to view your clusters workers, tasks and workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown the cluster\n",
    "To shutdown the cluster, simply call the `shutdown()` method. \n",
    "\n",
    "> **It is good practice to proactively shutdown your cluster when you have finished using it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <p>When you shut down your cluster, the machines that you started above will no longer be connected to this notebook, but they will continue to exist for another 10 minutes. This means that if you start up another remote cluster within the next 10 minutes, those machines will still be available and it will be much faster to start up. You will do exactly that a few cells below.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EASI utility functions\n",
    "Several utility functions are available to help manage your Dask cluster. The most useful is `initialize_dask()`, which will initialize either a local or remote cluster. \n",
    "\n",
    "In addition, the `localcluster_dashboard()` function can be used to return the dashboard address for your local Dask dashboard.\n",
    "\n",
    "Import these function from the `notebook_utils` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from notebook_utils import initialize_dask, localcluster_dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a local cluster\n",
    "To initialize a local dask cluster, simply set the `use_gateway=False` in the function call. No other variables are needed for a local cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster, client = initialize_dask(use_gateway=False)\n",
    "display(client)\n",
    "display(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a local cluster, you need to provide the server address of this Jupyter Hub environment. The `EasiDefaults` library contains a variable called `easi.hub` that will give you this URL easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(localcluster_dashboard(client, server=easi.hub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a local cluster running that you could use for data analysis. See the notebooks in the **/Dask** folder of this repository for more information on using local or remote dask clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Close the client\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a remote cluster\n",
    "\n",
    "But let's start up a cluster again. This shoudl run quickly if you shut down your previous remote cluster within the last 10 minutes and no other user has started a cluster in that time.\n",
    "\n",
    "To initialize a remote dask cluster using the `initialize_dask` utility function, set the `use_gateway=False` and specify the number of `workers` as a simple number. This value will determine the number of dask workers to start. Each default worker will have 8 cores and 28GiB of available RAM. If you set `wait=True`, and no compute nodes are available, this cell will only complete once all workers have started up. ___This may take several minutes.___\n",
    "\n",
    "This next cell will start a cluster with only one worker (`workers=1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster, client = initialize_dask(use_gateway=True, workers=1, wait=True)\n",
    "display(client)\n",
    "display(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Close the client and cluster to re-use the resources for the next example\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a remote cluster with adaptive scaling\n",
    "To initialize a remote dask cluster with adaptive scaling, set the `use_gateway=False` and specify the number of `workers` as a tuple (two numbers enclosed in parentheses) representing the minimum and maximum number of workers that you want in your cluster. If you set `wait=True`, this cell will only complete once the __minimum number of workers__ (the first value in `workers`) have started up. ___This may take several minutes if no nodes are already available.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster, client = initialize_dask(use_gateway=True, workers=(1,4), wait=True) # Ask for an adaptive cluster of between 1 and 4 workers\n",
    "display(client)\n",
    "display(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now that we have finished, fully shut down the cluster.\n",
    "client.close()\n",
    "cluster.close()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h4 class=\"alert-heading\">Note</h4>\n",
    "    <p>The tutorial notebooks in the <strong>/Dask</strong> folder in the root of this repository provide more information on how and why to use Dask, including information regarding chunking and a range of other Dask concepts.</p>\n",
    "    <p>Some basic information on Dask chunks is provided below.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask strategies\n",
    "\n",
    "As a processing job becomes large (large data or complex processing) a more considered approach to using dask may be necessary to achieve efficient and well-scaled processing. Here are some strategies that we've trialled:\n",
    "\n",
    "1. More workers *does not* mean faster. If more workers does not near-linearly increase your processing speed then try the next strategy.\n",
    "1. Restructure the data. Change the dask chunk size (and number of dask tasks) associated with your data objects depending on the type of processing being undertaken.\n",
    "   - The datacube loads data by spatial layers, so the initial `dask_chunks: {\"x\": 2048, \"y\": 2048}` in most `datacube.load()` operations is appropriate.\n",
    "   - If further processing (valid data, masking, scaling etc) are reasonably per pixel operations then dask chunks can remain as are (spatially-orientated).\n",
    "   - If time series processing then consider rechunking the xarray dask dataset on the time dimension.\n",
    "1. Consider GridWorkflow to break your large array into \"tiles\".\n",
    "   - The tiles are handled by dask independently of each other, thereby reducing the size of each dask dataset and processing request.\n",
    "   - You may need to consider node size as well including shared memory and CPU limits per node.\n",
    "   \n",
    "### Change the dask chunk size\n",
    "\n",
    "Use `display(xarray_dask_dataset)` to view the size of the proposed processing task. Select the <html><i class=\"fa fa-database\" style=\"font-size:18px;color:gray\"></i></html> icon and note the dask chunks size and number of tasks as shown below.\n",
    "\n",
    "<img width=\"50%\" src=\"../resources/dask-example.jpg\">\n",
    "\n",
    "> __Task limit__: We find that 500,000 tasks is about the limit of the dask scheduler.\n",
    "\n",
    "> __Chunk size__: Limit each chunk to less than 100 MB, 20-50 MB is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking examples\n",
    "\n",
    "From dask documentation's best practices https://docs.dask.org/en/latest/array-best-practices.html: \n",
    "\n",
    "> You want to choose a chunk size that is large in order to reduce the number of chunks that Dask has to think about (which affects overhead) but also small enough so that many of them can fit in memory at once. \n",
    "\n",
    "> The \"optimal\" chunk size can also depend on how the raw data is stored what operations you intend on performing on the dask Array.\n",
    "\n",
    "> In general, aiming for a resulting chunk size of a few hundred megabytes should work well. Things to consider include the data type and the dimensions.\n",
    "\n",
    "See also:\n",
    "- https://docs.dask.org/\n",
    "- https://xarray.pydata.org/\n",
    "- https://xarray.pydata.org/en/stable/dask.html\n",
    "- https://examples.dask.org/xarray.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a demonstration dataset with time, x and y dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "da = xr.DataArray(\n",
    "    np.random.rand(50,500,500),\n",
    "    dims = (\"time\",\"x\",\"y\"),\n",
    "    coords={\n",
    "        \"time\": pd.date_range(\"2000-01-01\", periods=50),\n",
    "        \"x\": np.random.randn(500),\n",
    "        \"y\": np.random.randn(500)\n",
    "    }\n",
    ")\n",
    "xarray_dask_dataset = da.chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chunking for spatial analysis - each time slice is a single chunk\n",
    "xarray_dask_rechunked = xarray_dask_dataset.chunk({'time':1})\n",
    "xarray_dask_rechunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rechunk an xarray dataset for time series processing - each full timeseries is in a chunk of 50x50 pixels\n",
    "# time:-1 means all time layers\n",
    "xarray_dask_rechunked = xarray_dask_dataset.chunk({'time':-1, 'x':50, 'y':50})\n",
    "xarray_dask_rechunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chunked into smaller \"cubes\"\n",
    "xarray_dask_rechunked = xarray_dask_dataset.chunk({'time':10, 'x':25, 'y':25})\n",
    "xarray_dask_rechunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
