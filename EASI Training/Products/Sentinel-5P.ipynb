{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-5P <img align=\"right\" src=\"../../resources/easi_logo.jpg\">\n",
    "\n",
    "#### Index\n",
    "- [Overview](#Overview)\n",
    "- [Setup (dask, imports, query)](#Setup)\n",
    "- [Product definition (measurements, flags)](#Product-definition)\n",
    "- [Quality layer (mask)](#Quality-layer)\n",
    "- [Scaling and nodata](#Scaling-and-nodata)\n",
    "- [Visualisation](#Visualisation)\n",
    "- [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "https://sentinel.esa.int/web/sentinel/missions/sentinel-5p\n",
    "\n",
    "The Copernicus Sentinel-5 Precursor mission is the first Copernicus mission dedicated to monitoring our atmosphere. The main objective of the Sentinel-5P mission is to perform atmospheric measurements with high spatio-temporal resolution, to be used for air quality, ozone & UV radiation, and climate monitoring & forecasting.\n",
    "\n",
    "The satellite was launched on 13 October 2017. The satellite's local time of ascending node crossing is 13.30 h, which was chosen to facilitate a loose formation with NASA's Suomi-NPP spacecraft.\n",
    "\n",
    "#### Data source and documentation\n",
    "\n",
    "Products of interest: CH4, CO, NO2 required at 7km, 7km pixel resolution, Australia coverage\n",
    "\n",
    "- https://sentinel.esa.int/web/sentinel/missions/sentinel-5p/data-products\n",
    "- https://sentinels.copernicus.eu/web/sentinel/technical-guides/sentinel-5p/products-algorithms\n",
    "\n",
    "| Name | Product | Information |\n",
    "|------|---------|-------------|\n",
    "| Methane (CH4) total column (HARP) | `cophub_s5p_ch4_harp` | - https://sentinels.copernicus.eu/documents/247904/2474726/Sentinel-5P-Level-2-Product-User-Manual-Methane<br>- https://stcorp.github.io/harp/doc/html/ingestions/S5P_L2_CH4.html |\n",
    "| Carbon Monoxide (CO) total column (HARP) | `cophub_s5p_co_harp` | - https://sentinels.copernicus.eu/documents/247904/2474726/Sentinel-5P-Level-2-Product-User-Manual-Carbon-Monoxide<br>- https://stcorp.github.io/harp/doc/html/ingestions/S5P_L2_CO.html |\n",
    "| Nitrogen Dioxide (NO2), total and tropospheric columns (HARP) | `cophub_s5p_no2_harp` | - https://sentinels.copernicus.eu/documents/247904/2474726/Sentinel-5P-Level-2-Product-User-Manual-Nitrogen-Dioxide<br>- https://stcorp.github.io/harp/doc/html/ingestions/S5P_L2_NO2.html |\n",
    "| Other | |\n",
    "| - Cloud fraction, albedo, top pressure | | https://sentinel.esa.int/documents/247904/2474726/Sentinel-5P-Level-2-Product-User-Manual-NPP-Cloud-product |\n",
    "| - UV Aerosol Index | | https://sentinels.copernicus.eu/documents/247904/2474726/Sentinel-5P-Level-2-Product-User-Manual-Aerosol-Index-product |\n",
    "| - Aerosol Layer Height (mid-level pressure) | | https://sentinels.copernicus.eu/documents/247904/2474726/Sentinel-5P-Level-2-Product-User-Manual-Aerosol-Layer-Height |\n",
    "\n",
    "HARP, https://stcorp.github.io/harp/doc/html/index.html\n",
    "- HARP filters and converts data: remove unneeded parameters/measurements, add derived physical parameters, perform unit conversion, regrid dimensions\n",
    "\n",
    "Level-2 products are:\n",
    "- geolocated total columns of ozone, sulfur dioxide, nitrogen dioxide, carbon monoxide, formaldehyde and methane\n",
    "- geolocated tropospheric columns of ozone\n",
    "- geolocated vertical profiles of ozone\n",
    "- geolocated cloud and aerosol information (e.g. absorbing aerosol index and aerosol layer height)\n",
    "\n",
    "#### Reference datasets\n",
    "\n",
    "__Google Earth Engine__\n",
    "\n",
    "https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CH4\n",
    "\n",
    "The original Sentinel 5P Level 2 (L2) data is binned by time, not by latitude/longitude. To make it possible to ingest the data into Earth Engine, each Sentinel 5P L2 product is converted to L3, keeping a single grid per orbit (that is, no aggregation across products is performed).\n",
    "\n",
    "Source products spanning the antimeridian are ingested as two Earth Engine assets, with suffixes _1 and _2.\n",
    "\n",
    "The conversion to L3 is done by the harpconvert tool using the bin_spatial operation. The source data is filtered to remove pixels with QA values less than:\n",
    "- 80% for AER_AI\n",
    "- 75% for the tropospheric_NO2_column_number_density band of NO2\n",
    "- 50% for all other datasets except for O3 and SO2\n",
    "- The O3_TCL product is ingested directly (without running harpconvert).\n",
    "\n",
    "__OpenSearch / GES-DISC__\n",
    "\n",
    "https://cmr.earthdata.nasa.gov/opensearch > Granule search\n",
    "\n",
    "| OpenSearch Short Name | Description |\n",
    "|------------|-------------|\n",
    "| ```S5P_L2__AER_AI_HiR``` | S5P Aerosol Index L2 5.5km x 3.5km |\n",
    "| ```S5P_L2__AER_LH_HiR``` | S5P Aerosol Layer Height L2 5.5km x 3.5km |\n",
    "| ```S5P_L2__CH4____HiR``` | S5P Methane CH4 L2 5.5km x 7km |\n",
    "| ```S5P_L2__CLOUD__HiR``` | S5P Cloud L2 5.5km x 3.5km |\n",
    "| ```S5P_L2__CO_____HiR``` | S5P Carbon Monoxide CO Column L2 5.5km x 7km |\n",
    "| ```S5P_L2__NO2____HiR``` | S5P Tropospheric NO2 L2 5.5km x 3.5km |\n",
    "\n",
    "#### EASI pipeline\n",
    "\n",
    "| Task | Summary |\n",
    "|------|---------|\n",
    "| Source | Copernicus Science Hub<br>- https://s5phub.copernicus.eu/dhus/<br>- User: s5pguest \\| Pass: s5pguest |\n",
    "| Download | - Modified version of https://github.com/sentinelsat/sentinelsat<br>- Same data-pipeline code as for the [CopHub Sentinel-2 product](Sentinel-2.ipynb) |\n",
    "| Preprocess | - Uses HARP (mostly following https://github.com/bilelomrani1/s5p-tools)<br>- Intention is to be reasonably equivalent to the GEE products |\n",
    "| Format | Convert to COGs |\n",
    "| Prepare | EO3 metadata taken from S5P netcdf file |\n",
    "| TODO | - Decide whether (GEE/)HARP parameters and variables are fit for purpose<br>- Convert from HARP netcdf to Zarr |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "#### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://10.0.98.79:45973\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data tools\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Datacube\n",
    "import datacube\n",
    "from datacube.utils import masking  # https://github.com/opendatacube/datacube-core/blob/develop/datacube/utils/masking.py\n",
    "from odc.algo import enum_to_bool   # https://github.com/opendatacube/odc-tools/blob/develop/libs/algo/odc/algo/_masking.py\n",
    "from odc.algo import xr_reproject   # https://github.com/opendatacube/odc-tools/blob/develop/libs/algo/odc/algo/_warp.py\n",
    "from datacube.utils.geometry import GeoBox, box  # https://github.com/opendatacube/datacube-core/blob/develop/datacube/utils/geometry/_base.py\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "\n",
    "# Holoviews, Datashader and Bokeh\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "import colorcet as cc\n",
    "import cartopy.crs as ccrs\n",
    "from datashader import reductions\n",
    "from holoviews import opts\n",
    "# import geoviews as gv\n",
    "# from holoviews.operation.datashader import rasterize\n",
    "hv.extension('bokeh', logo=False)\n",
    "\n",
    "# Python\n",
    "import sys, os, re\n",
    "\n",
    "# Optional EASI tools\n",
    "sys.path.append(os.path.expanduser('~/hub-notebooks/scripts'))\n",
    "import notebook_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ODC develop credentials\n",
    "\n",
    "This is a development ODC database while we test and demo this product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF = \"\"\"\n",
    "[datacube]\n",
    "db_hostname: v2-db-easihub-csiro-eks.cluster-ro-cvaedcg0qvwd.ap-southeast-2.rds.amazonaws.com\n",
    "db_database: user_dev_odc\n",
    "db_username: user\n",
    "db_password: secretpassword\n",
    "\"\"\"\n",
    "from datacube.config import read_config, LocalConfig\n",
    "dc = datacube.Datacube(config=LocalConfig(read_config(CONF)), env='datacube')\n",
    "\n",
    "# dc = datacube.Datacube()  # Uncomment when ODC develop credentials are no longer required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Australia\n",
    "min_longitude, max_longitude = (110, 154)\n",
    "min_latitude, max_latitude = (-45, -10)\n",
    "min_date = '2019-01-01'\n",
    "max_date = '2021-03-31'\n",
    "product = 'cophub_s5p_ch4_harp'\n",
    "\n",
    "native_crs = 'epsg:4326'\n",
    "\n",
    "query = {\n",
    "    'product': product,                     # Product name\n",
    "    'x': (min_longitude, max_longitude),    # \"x\" axis bounds\n",
    "    'y': (min_latitude, max_latitude),      # \"y\" axis bounds\n",
    "    'time': (min_date, max_date),           # Any parsable date strings\n",
    "    'output_crs': native_crs,               # EPSG code\n",
    "    'resolution': (0.01, 0.01),             # Target resolution\n",
    "    'group_by': 'solar_day',                # Scene ordering\n",
    "    'dask_chunks': {'latitude': 2048, 'longitude': 2048},  # Dask chunks\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional. Some products require AWS S3 credentials to supplied\n",
    "\n",
    "# S3 credentials - required for s2_l2a\n",
    "# configure_s3_access(aws_unsigned=True,requester_pays=False,client=client)\n",
    "# print(\"Configured s3 requester pays data access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = dc.load(**query)\n",
    "\n",
    "notebook_utils.heading(notebook_utils.xarray_object_size(data))\n",
    "display(data)\n",
    "\n",
    "# Calculate valid (not nodata) masks for each layer\n",
    "valid_mask = masking.valid_data_mask(data)\n",
    "notebook_utils.heading('Valid data masks for each variable')\n",
    "display(valid_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product definition\n",
    "\n",
    "Display the measurement definitions for the selected product.\n",
    "\n",
    "Use `list_measurements` to show the details for a product, and `masking.describe_variable_flags` to show the flag definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurement definitions for the selected product\n",
    "measurement_info = dc.list_measurements().loc[query['product']]\n",
    "notebook_utils.heading(f'Measurement table for product: {query[\"product\"]}')\n",
    "notebook_utils.display_table(measurement_info)\n",
    "\n",
    "# Separate lists of measurement names and flag names\n",
    "measurement_names = measurement_info[ pd.isnull(measurement_info.flags_definition)].index\n",
    "flag_names        = measurement_info[pd.notnull(measurement_info.flags_definition)].index\n",
    "\n",
    "notebook_utils.heading('Selected Measurement and Flag names')\n",
    "notebook_utils.display_table(pd.DataFrame({\n",
    "    'group': ['Measurement names', 'Flag names'],\n",
    "    'names': [', '.join(measurement_names), ', '.join(flag_names)]\n",
    "}))\n",
    "\n",
    "# Flag definitions\n",
    "for flag in flag_names:\n",
    "    notebook_utils.heading(f'Flag definition table for flag name: {flag}')\n",
    "    notebook_utils.display_table(masking.describe_variable_flags(data[flag]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality layer\n",
    "\n",
    "S5P HARP products have been pre-processed with a quality filter\n",
    "\n",
    "| product | filter |\n",
    "|---------|--------|\n",
    "| cophub_s5p_ch4_harp | CH4_column_volume_mixing_ratio_dry_air_validity >= 50 |\n",
    "| cophub_s5p_co_harp | CO_column_number_density_validity >= 50 |\n",
    "| cophub_s5p_no2_harp | tropospheric_NO2_column_number_density_validity >= 75<br>tropospheric_NO2_column_number_density >= 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and nodata\n",
    "\n",
    "S5P HARP products have been preprocessed to floating-point values, and presumably have been scaled (opaque documentation).\n",
    "\n",
    "The nodata value was set to -9999 for each band. This will have been applied at `valid_mask = masking.valid_data_mask(data)` just after `dc.load()` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a layer and apply masking and scaling, then persist in dask\n",
    "\n",
    "layer_name = 'ch4_column_volume_mixing_ratio_dry_air'  # cophub_s5p_ch4_harp\n",
    "# layer_name = 'co_column_number_density'                # cophub_s5p_co_harp\n",
    "# layer_name = 'tropospheric_no2_column_number_density'    # cophub_s5p_no2_harp\n",
    "\n",
    "# Apply valid mask and good pixel mask\n",
    "layer = data[layer_name].where(valid_mask[layer_name])\n",
    "layer = layer.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a plot\n",
    "\n",
    "options = {\n",
    "    'title': f'{query[\"product\"]}: {layer_name}',\n",
    "    'width': 700,\n",
    "    'height': 450,\n",
    "    'aspect': 'equal',\n",
    "    'cmap': cc.rainbow,\n",
    "    'clim': (1800, 2000),   # CH4 .. 1750 = pre-industrial\n",
    "# #     'clim': (0, 0.05),      # CO\n",
    "#     'clim': (0, 1e-4),        # NO2\n",
    "    'colorbar': True,\n",
    "    'tools': ['hover'],\n",
    "}\n",
    "\n",
    "# Set the Dataset CRS\n",
    "plot_crs = native_crs\n",
    "if plot_crs == 'epsg:4326':\n",
    "    plot_crs = ccrs.PlateCarree()\n",
    "\n",
    "\n",
    "# Native data and coastline overlay:\n",
    "# - Comment `crs`, `projection`, `coastline` to plot in native_crs coords\n",
    "# TODO: Update the axis labels to 'longitude', 'latitude' if `coastline` is used\n",
    "    \n",
    "layer_plot = layer.hvplot.image(\n",
    "    x = 'longitude', y = 'latitude',         # Dataset x,y dimension names\n",
    "    rasterize = True,                        # Use Datashader\n",
    "    aggregator = reductions.mean(),          # Datashader selects mean value\n",
    "    precompute = True,                       # Datashader precomputes what it can\n",
    "    crs = plot_crs,                          # Dataset crs\n",
    "    projection = ccrs.PlateCarree(),         # Output projection (use ccrs.PlateCarree() when coastline=True)\n",
    "    coastline='10m',                         # Coastline = '10m'/'50m'/'110m'\n",
    ").options(opts.Image(**options)).hist(bin_range = options['clim'])\n",
    "\n",
    "# display(layer_plot)\n",
    "# Optional: Change the default time slider to a dropdown list, https://stackoverflow.com/a/54912917\n",
    "fig = pn.panel(layer_plot, widgets={'time': pn.widgets.Select})  # widget_location='top_left'\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
